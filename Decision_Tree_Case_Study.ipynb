{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1ohzpK4-uhJS",
        "rXvWujrUum9N"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tripathiosho/ImportantNotebooks/blob/main/Decision_Tree_Case_Study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing the Steel Plant Faults with Decision Trees\n",
        "\n",
        "## Context\n",
        "\n",
        "Welcome to the **Scaler Industrial** Development data analysis team! Our current project aims to improve the quality control process in steel manufacturing. Your expertise is requested to analyze the Steel Plant Faults dataset, utilizing Decision Trees to classify different types of steel plate faults. This task will help in automating the identification of defects and ensuring high standards in steel production.\n",
        "\n",
        "## Dataset Description\n",
        "\n",
        "The dataset you will analyze includes a variety of attributes that describe defects found in steel plates. These defects are classified into seven distinct categories, each representing a specific type of fault:\n",
        "\n",
        "### Steel Plate Fault Types:\n",
        "1. **Pastry:** Small patches or irregularities on the surface of the steel plate.\n",
        "2. **Z_Scratch:** Narrow, parallel scratches or marks in the direction of rolling.\n",
        "3. **K_Scratch:** Scratches similar to Z-scratches, but perpendicular to the rolling direction.\n",
        "4. **Stains:** Discolored or contaminated areas on the steel surface.\n",
        "5. **Dirtiness:** Presence of dirt or particulate matter on the steel surface.\n",
        "6. **Bumps:** Raised or protruding areas on the steel surface.\n",
        "7. **Other_Faults:** A broad category for faults not specifically mentioned in the other types.\n"
      ],
      "metadata": {
        "id": "Ow6YbSrgY5sy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM-aeWSQYi6Y"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "!wget https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/071/024/original/SteelPlant.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "7Vxn4MtKotMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"SteelPlant.csv\")\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "xEVbaZtuY30E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assesment for DT Lecture 1"
      ],
      "metadata": {
        "id": "1ohzpK4-uhJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Decision Tree parametric algo\n",
        "\n",
        "#### Task:  \n",
        "Before we can utilize DT to classify steel plate faults, let's revisit the fundamental characteristics of the algorithm.\n",
        "\n",
        "#### Question:  \n",
        "Is the Decision Tree a parametric algorithm?\n",
        "\n",
        "#### Options:\n",
        "\n",
        "A] Yes\n",
        "\n",
        "B] No"
      ],
      "metadata": {
        "id": "5a2krDSxbc09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Root Entropy for Decision Tree\n",
        "\n",
        "#### Context:\n",
        "In a Decision Tree classifier, entropy is a crucial measure used to determine the impurity of a node. In the context of the Steel Plant Faults dataset, understanding how to calculate the root entropy will provide insights into the initial uncertainty before any splits.\n",
        "\n",
        "#### Task:\n",
        "Write Python code to transform one-hot encoded fault types into a single 'Target' column and calculate the root entropy. This process involves aggregating multiple binary fault type columns into a single categorical column and then applying the entropy formula.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Create the 'Target' Column:** Combine the one-hot encoded fault type columns ('Pastry', 'Z_Scratch', 'K_Scratch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults') into a single 'Target' column that contains the fault type with the maximum value for each row.\n",
        "2. **Entropy Calculation Function:** Define a function `entropy_calc` that calculates the entropy for the 'Target' column, utilizing the probabilities of each fault type.\n",
        "3. **Compute Root Entropy:** Apply the `entropy_calc` function to the 'Target' column to find the root entropy and print the result.\n",
        "\n",
        "#### Question:\n",
        "For this task, how do you transform the one-hot encoded fault types into a single 'Target' column and calculate the root entropy?\n",
        "\n"
      ],
      "metadata": {
        "id": "gSVPyLXhdo-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "fault_types = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']\n",
        "df['Target'] = df[fault_types].idxmax(axis=1)\n",
        "\n",
        "# TODO: Function to calculate entropy\n",
        "def entropy_calc(y):\n",
        "    probabilities = y._________(normalize=True).to_numpy() # Hint: Pandas function to count the frequency for each category.\n",
        "    entropies = -_____ * np.log2(_____)  # Using log base 2 for entropy calculation\n",
        "    return round(np.____(entropies), 2)\n",
        "\n",
        "# Calculate root entropy\n",
        "root_entropy = entropy_calc(df['Target'])\n",
        "print(f\"The root entropy is: {root_entropy}\")"
      ],
      "metadata": {
        "id": "wDGrY8MndokY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Preprocessing for DT\n",
        "\n",
        "\n",
        "#### Context:\n",
        "In the preparation phase for applying a Decision Tree (DT) classifier to the Steel Plant Faults dataset, various preprocessing steps are considered. It's crucial to identify which of these steps is unnecessary for DT, given its unique properties and how it handles data.\n",
        "\n",
        "#### Question:\n",
        "Which preprocessing step can be generally avoided when implementing a Decision Tree for the Steel Plant Faults dataset, where features are on different scales?\n",
        "\n",
        "#### Options:\n",
        "A) Feature Selection\n",
        "\n",
        "B) Feature Scaling\n",
        "\n",
        "C) SMOTE\n",
        "\n",
        "D) Encoding\n",
        "\n"
      ],
      "metadata": {
        "id": "J5rnU4GSmBku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "## Understanding Decision Tree Characteristics\n",
        "\n",
        "#### Context:\n",
        "In building a Decision Tree (DT) for classification, various concepts and methods play a crucial role in the tree's construction and decision-making process. Understanding these aspects is essential for effectively applying DT in practical scenarios.\n",
        "\n",
        "#### Question:\n",
        "Which of the following statements are correct regarding the construction and characteristics of a Decision Tree?\n",
        "\n",
        "#### Statements to Evaluate:\n",
        "S1: Fill the missing attribute value with the Central Tendency (mean, median, mode) of the attribute.\n",
        "\n",
        "S2: Decision Trees can handle both numerical and categorical data.\n",
        "\n",
        "S3: The root Node is the top-most node of the Tree from where the Tree starts.\n",
        "\n",
        "S4: Information gain is the difference between the entropy of the parent and the weighted average entropy of the children.\n",
        "\n",
        "#### Options:\n",
        "A) S1, S2, S4\n",
        "\n",
        "B) S2, S3, S4\n",
        "\n",
        "C) S1, S2, S3\n",
        "\n",
        "D) S1, S2, S3, S4\n",
        "\n"
      ],
      "metadata": {
        "id": "DRO_9Zq8mN-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Assessing Dataset Balance for Decision Tree Implementation\n",
        "\n",
        "#### Context:\n",
        "Understanding the distribution of classes in a dataset is essential for machine learning tasks, particularly for classification. The balance or imbalance of the dataset can influence the performance and decision-making process of a Decision Tree (DT) classifier.\n",
        "\n",
        "#### Task:\n",
        "You are asked to write a Python script that visualizes the distribution of classes in the Steel Plant Faults dataset to assess its balance. Based on this analysis, you will determine the necessity of applying imbalance-handling techniques when implementing a DT.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Visualize Class Distribution:** Create a bar plot to visualize the count of each fault type ('Pastry', 'Z_Scratch', 'K_Scratch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults') in the dataset.\n",
        "2. **Analyze the Plot:** Examine the bar plot to assess whether the dataset is balanced or imbalanced across the different fault types.\n",
        "3. **Decision Tree Consideration:** Decide whether a DT classifier would require imbalance-handling techniques based on the class distribution.\n",
        "\n",
        "#### Question:\n",
        "Based on your analysis of the class distribution in the Steel Plant Faults dataset, is the dataset balanced, and should an imbalance-handling algorithm be applied when implementing a Decision Tree?\n",
        "\n",
        "#### Options:\n",
        "A) The dataset is balanced, and Decision Trees are not affected by data imbalance.\n",
        "\n",
        "B) The dataset is imbalanced, but Decision Trees are not significantly affected by data imbalance.\n",
        "\n",
        "C) The dataset is balanced, but Decision Trees are affected by data imbalance.\n",
        "\n",
        "D) The dataset is imbalanced, and Decision Trees are affected by data imbalance."
      ],
      "metadata": {
        "id": "LxPNGGlSn7s9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fault_types = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']\n",
        "\n",
        "# TODO: Count number of data points for each fault type\n",
        "fault_counts = _____\n",
        "\n",
        "# Set a custom color palette (optional)\n",
        "custom_palette = sns.color_palette(\"pastel\")\n",
        "\n",
        "# TODO: Create the bar plot to visualize the count of each fault type\n",
        "plt.figure(figsize=(8, 6))  # Adjust figure size\n",
        "sns.barplot(x=____, y=____.to_numpy(), palette=custom_palette)\n",
        "\n",
        "# Customize labels and title\n",
        "plt.xlabel(\"Classes\", fontsize=12)\n",
        "plt.ylabel(\"Counts\", fontsize=12)\n",
        "plt.title(\"Distribution of Classes\", fontsize=14)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()  # Ensure labels fit within the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z4bs6xYJiGIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Find Root Node\n",
        "\n",
        "#### Context:\n",
        "In a Decision Tree, the root node is pivotal as it represents the initial decision point from which further splits are derived. The Gini value at this node offers insight into the node's purity. This exercise involves training a Decision Tree using only two features from the Steel Plant Faults dataset and determining the root node's feature and its Gini value.\n",
        "\n",
        "#### Task:\n",
        "Train a Decision Tree classifier using 'Pixels_Areas' and 'SigmoidOfAreas' as features. Set the `max_depth` to 3 and `random_state` to 42. Determine which feature is chosen as the root node and identify its Gini value.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Feature Selection:** Use 'Pixels_Areas' and 'SigmoidOfAreas' as your features for the Decision Tree.\n",
        "2. **Data Splitting:** Split the dataset into training and testing sets.\n",
        "3. **Model Initialization:** Initialize a Decision Tree Classifier with `max_depth=3` and `random_state=42`.\n",
        "4. **Model Training:** Train the Decision Tree on the training data.\n",
        "5. **Root Node Identification:** Identify the feature used at the root node and find its Gini value.\n",
        "\n",
        "#### Question:\n",
        "After training the Decision Tree with the specified hyperparameters using 'Pixels_Areas' and 'SigmoidOfAreas' as features, which feature becomes the root node, and what is its Gini value?\n",
        "\n",
        "#### Options:\n",
        "A) 'Pixels_Areas' is the root node, with a Gini value of X.\n",
        "\n",
        "B) 'SigmoidOfAreas' is the root node, with a Gini value of Y.\n",
        "\n",
        "C) Both 'Pixels_Areas' and 'SigmoidOfAreas' share the root node, with Gini values of X and Y respectively.\n",
        "\n",
        "D) Neither 'Pixels_Areas' nor 'SigmoidOfAreas' becomes the root node."
      ],
      "metadata": {
        "id": "inRu78UQzFEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.____ import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# TODO: Selecting the two features for training\n",
        "X = _____\n",
        "y = df['Target']\n",
        "\n",
        "# TODO: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = _____(X, y, random_state=42)\n",
        "\n",
        "# TODO: Initialize the Decision Tree Classifier with maximum depth if 3 and random_state=42\n",
        "dt_classifier = DecisionTreeClassifier(_____, random_state=42)\n",
        "\n",
        "# TODO: Train the Decision Tree Classifier on the training data\n",
        "dt_classifier.___(_____, _____)\n",
        "\n",
        "# TODO: Getting the root node feature name and its Gini value\n",
        "# Hint - use documentation to find the property to access the values.\n",
        "root_node_feature_index = dt_classifier.____.feature[0]\n",
        "root_node_feature_name = X.columns[root_node_feature_index]\n",
        "root_node_gini_value = dt_classifier.____.____[0]\n",
        "\n",
        "print(f\"The root node feature is: {root_node_feature_name}\")\n",
        "print(f\"The Gini value of the root node is: {root_node_gini_value:.2f}\")"
      ],
      "metadata": {
        "id": "L-lqBteVoHJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Assesment for DT Lecture 2"
      ],
      "metadata": {
        "id": "rXvWujrUum9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "## DT- Root Split\n",
        "\n",
        "#### Context:  \n",
        "Understanding the sensitivity of a Decision Tree's structure to changes in the training data can provide insights into the model's stability and robustness. Removing a data point from the dataset may affect the tree's formation, particularly the feature and threshold selected for the root node's split.\n",
        "\n",
        "#### Task:  \n",
        "Consider a scenario where you remove one data point from the dataset used to train a Decision Tree.\n",
        "\n",
        "#### Question:  \n",
        "Which option is true regarding the splitting criteria at the root of the tree if one of the data points is removed?\n",
        "\n",
        "#### Options:  \n",
        "A) The splitting criteria at the root will be the same.  \n",
        "B) The splitting criteria at the root will be different.  \n",
        "C) The splitting criteria at the root could be the same or different.  \n",
        "D) The splitting criteria at the root can’t be related to this removal of the data point.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pzYHNsmMuWiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Most Misclassified DT\n",
        "\n",
        "#### Context:\n",
        "Using class weights in a Decision Tree can help address class imbalance by adjusting the decision criteria toward less represented classes. This task involves applying class weights to a Decision Tree classifier and identifying the class that is most frequently misclassified.\n",
        "\n",
        "#### Task:\n",
        "Write Python code to train a Decision Tree classifier using entropy as the criterion and balanced class weights. Your goal is to determine which class is most misclassified on the test dataset.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Feature and Target Selection:** Prepare your features `X` by dropping the class columns and 'Target', and set `y` as your 'Target'.\n",
        "2. **Data Splitting:** Split the dataset into training and testing sets using `train_test_split` with `random_state=42`.\n",
        "3. **Class Weights Calculation:** Use the 'balanced' option for class weights to handle class imbalance.\n",
        "4. **Initialize and Train Decision Tree:** Create a Decision Tree classifier with entropy criterion, incorporating the calculated class weights. Train this classifier on your training data.\n",
        "5. **Model Prediction and Evaluation:** Use the trained model to predict the classes on the test data. Generate a classification report to evaluate the model's performance across different classes.\n",
        "6. **Identify Most Misclassified Class:** Analyze the recall values from the classification report to identify which class is the most misclassified.\n",
        "\n",
        "#### Question:\n",
        "After applying the described Decision Tree configuration and analyzing the classification report, which class is identified as the most misclassified?\n",
        "\n",
        "#### Options:\n",
        "A) Bumps\n",
        "\n",
        "B) Other_Faults\n",
        "\n",
        "C) Dirtiness\n",
        "\n",
        "D) Pastry\n",
        "\n"
      ],
      "metadata": {
        "id": "tY_Y08WHuuTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "ry4g8k9Q01DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.____ import classification_report\n",
        "\n",
        "# Prepare our features for training\n",
        "X = df.drop(columns=['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains',\n",
        "       'Dirtiness', 'Bumps', 'Other_Faults', 'Target'], axis =1)\n",
        "y = df['Target']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# TODO: Initialize the Decision Tree classifier with the given hyperparameters and class weights\n",
        "# Hint: Use documentation to find how to balance the class_weights in DecisionTrees\n",
        "dt_classifier = DecisionTreeClassifier(____='entropy', random_state=42, class_weight=____)\n",
        "\n",
        "# TODO: Train the Decision Tree classifier on training data\n",
        "dt_classifier.___(X_train, ____)\n",
        "\n",
        "# TODO: Predict on the test data\n",
        "y_pred = dt_classifier.____(____)\n",
        "\n",
        "# TODO: Generate a classification report to include the metrics for each class\n",
        "report = classification_report(___, ____, target_names=np.unique(y_train))\n",
        "print(report)"
      ],
      "metadata": {
        "id": "ORVdFsFIvrBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Splitting Data\n",
        "\n",
        "#### Context:\n",
        "In Decision Trees, achieving a node where each child contains samples exclusively from one class represents an ideal split where classes are perfectly separated. Such scenarios prompt specific decisions on how to proceed with further splits or terminate the tree building process.\n",
        "\n",
        "#### Task:\n",
        "You have a Decision Tree where the feature `X_minimum` leads to a node split such that each of its 7 children contains samples belonging exclusively to one class. Decide the next steps in the tree construction process.\n",
        "\n",
        "#### Question:\n",
        "If using the feature `X_minimum` results in each child node containing samples from exactly one class, what should you do next?\n",
        "\n",
        "#### Options:\n",
        "A) Find another feature to split the node further.\n",
        "\n",
        "B) Declare it as a leaf node since it is a pure node.\n",
        "\n",
        "C) Always terminate the recursions on all branches and return the current tree.\n",
        "\n",
        "D) Go back to the parent node and select a different feature to split the data so that y-values are not all the same at this node.\n"
      ],
      "metadata": {
        "id": "Uhga3yhe5T8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Decision Trees Characteristics\n",
        "\n",
        "#### Context:  \n",
        "Decision Trees are a fundamental machine learning algorithm used for both classification and regression tasks. Understanding their characteristics, capabilities, and limitations is crucial for effectively applying them to solve real-world problems.\n",
        "\n",
        "#### Question:  \n",
        "Which of the following statements are true regarding the properties and behavior of Decision Trees?\n",
        "\n",
        "#### Statements to Evaluate:  \n",
        "1\\. Decision tree makes no assumptions about the data.  \n",
        "2\\. The decision tree model can learn non-linear decision boundaries.  \n",
        "3\\. Decision trees cannot explain how the target will change if a variable is changed by 1 unit (marginal effect).  \n",
        "4\\. Hyperparameter tuning is not required in decision trees.  \n",
        "5\\. In a decision tree, increasing entropy implies increasing purity.  \n",
        "6\\. In a decision tree, the entropy of a node decreases as we go down the decision tree.\n",
        "\n",
        "#### Options:  \n",
        "A) 1, 2, and 5  \n",
        "B) 3, 5, and 6  \n",
        "C) 2, 3, 4, and 5  \n",
        "D) 1, 2, 3, and 6"
      ],
      "metadata": {
        "id": "U2efMO6j8VXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "## Optimizing Decision Tree Depth\n",
        "\n",
        "#### Context:\n",
        "Cross-validation is an essential technique for assessing the generalizability of machine learning models. In this task, you will use k-fold cross-validation to determine the optimal depth for a Decision Tree classifier that maximizes accuracy on a given dataset.\n",
        "\n",
        "#### Task:\n",
        "Write Python code to perform k-fold cross-validation on a series of Decision Tree models with varying depths. The goal is to identify the tree depth that results in the best performance on the test data.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Define Depth Values:** Use the list of depths `[3, 5, 7, 11, 13, 15, 19, 20]` to evaluate which tree depth provides the best model performance.\n",
        "2. **Initialize K-Fold Cross-Validation:** Set up a `KFold` instance with 10 splits and `random_state=42` to ensure reproducibility.\n",
        "3. **Cross-Validation Loop:** For each depth in your list, initialize a Decision Tree with `class_weight='balanced'`. Perform cross-validation and record the training and validation accuracies.\n",
        "4. **Determine Best Depth:** Analyze the cross-validation results to select the depth that provides the highest validation accuracy.\n",
        "5. **Train and Evaluate the Best Model:** Using the identified best depth, train a new Decision Tree on the entire training set and evaluate its performance on the test set.\n",
        "\n",
        "#### Question:\n",
        "Based on your cross-validation results, which depth should be selected for the Decision Tree to achieve optimal performance, and what is the test accuracy for this depth?"
      ],
      "metadata": {
        "id": "sCkDaYHm9K8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn._____ import KFold, cross_validate\n",
        "\n",
        "# Define the list of tree depths you want to evaluate\n",
        "depths = [3, 5, 7, 11, 13, 15, 19, 20]\n",
        "\n",
        "# TODO: Setup KFold with 10 splits and a fixed random state\n",
        "kfold = ___(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "for depth in depths:\n",
        "    # TODO: Initialize Decision Tree Classifier with the current depth\n",
        "    tree_clf = DecisionTreeClassifier(____=depth, random_state=42, class_weight='balanced')\n",
        "\n",
        "    # TODO: Perform cross-validation\n",
        "    cv_acc_results = cross_validate(____, X_train, ____, cv=_____, scoring='accuracy', return_train_score=True)\n",
        "\n",
        "    # Output the mean and standard deviation of the training and validation scores\n",
        "    print(f\"K-Fold for depth: {depth} - Accuracy Mean: Train: {cv_acc_results['train_score'].mean() * 100:.2f}%, Validation: {cv_acc_results['test_score'].mean() * 100:.2f}%\")\n",
        "    print(f\"K-Fold for depth: {depth} - Accuracy Std: Train: {cv_acc_results['train_score'].std() * 100:.2f}%, Validation: {cv_acc_results['test_score'].std() * 100:.2f}%\")\n",
        "    print('***************')\n",
        "\n",
        "# Testing the best depth on the testing data\n",
        "best_tree_clf = DecisionTreeClassifier(max_depth=5, random_state=42, class_weight='balanced')\n",
        "best_tree_clf.fit(X_train, y_train)\n",
        "print(f\"The model accuracy on testing data with best depth is {best_tree_clf.score(X_test, y_test)}\")"
      ],
      "metadata": {
        "id": "oZ8WjnwixQt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## DT - Feature Importance\n",
        "\n",
        "#### Context:\n",
        "Understanding which features most significantly impact the predictions of a Decision Tree is crucial for refining models and enhancing interpretability. This task involves identifying the top three features based on their importance scores from a trained Decision Tree.\n",
        "\n",
        "#### Task:\n",
        "Write Python code to analyze the feature importance from a Decision Tree (`best_tree_clf`) and identify the top three most influential features.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Retrieve Feature Importances:** Extract the importance scores of all features from the `best_tree_clf`.\n",
        "2. **Rank Features by Importance:** Sort the features by their importance in descending order.\n",
        "3. **Identify Top Three Features:** Determine the three features with the highest importance scores.\n",
        "4. **Visualize Feature Importance:** Optionally, create a bar plot to visually represent the importance of each feature, which can aid in understanding their relative contributions to the model's decisions.\n",
        "\n",
        "#### Question:\n",
        "After executing your script to analyze feature importances in the Decision Tree, which three features are identified as the most influential based on their importance scores?\n",
        "\n",
        "#### Options:\n",
        "A) `Outside_X_Index`, `Pixels_Areas`, `Length_of_Conveyer`\n",
        "\n",
        "B) `X_Minimum`, `Y_Maximum`, `Sum_of_Luminosity`\n",
        "\n",
        "C) `Maximum_of_Luminosity`, `Edges_Index`, `Steel_Plate_Thickness`\n",
        "\n",
        "D) `TypeOfSteel_A300`, `Edges_Y_Index`, `LogOfAreas`\n",
        "\n"
      ],
      "metadata": {
        "id": "FUq956RvB5Hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = X_train.columns\n",
        "\n",
        "# TODO: Get feature importances of the best_tree_clf from 'previous question'\n",
        "feature_importances = ______\n",
        "\n",
        "# TODO: Sort the features by importance (get their indices only not the values. )\n",
        "sorted_idx = np.____(feature_importances)[::-1]  # This sorts indices by importance from high to low\n",
        "\n",
        "# Output the top 3 features\n",
        "print(\"Top 3 features:\")\n",
        "for i in range(3):\n",
        "    feature_index = sorted_idx[i]\n",
        "    print(f\"{i+1}. {feature_names[feature_index]} with an importance of {feature_importances[feature_index]:.4f}\")\n",
        "\n",
        "sorted_feature_names = [feature_names[i] for i in sorted_idx]\n",
        "sorted_importances = feature_importances[sorted_idx]\n",
        "\n",
        "# TODO: Create a horizontal bar plot for the feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(range(len(sorted_importances)), _____, align='center', color='dodgerblue')\n",
        "plt.yticks(range(len(sorted_importances)), sorted_feature_names)\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature Names')\n",
        "plt.title('Feature Importance from Decision Tree')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to have the highest importance on top\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MaiJwubO-uGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Assesment for Bagging\n"
      ],
      "metadata": {
        "id": "fqR-fRBMFXcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Role of Ensemble Methods\n",
        "\n",
        "\n",
        "#### Question:\n",
        "Choose the correct statements concerning the use of ensemble algorithms with Decision Trees in machine learning.\n",
        "\n",
        "#### Statements to Evaluate:\n",
        "S1: To improve our Decision Tree model performance, an ensemble algorithm is used.\n",
        "S2: The Decision Tree model in our dataset was overfitting, hence an ensemble algorithm was used.\n",
        "S3: Ensemble uses many independent models and aggregates their output.\n",
        "S4: The Decision Tree model in our dataset was underfitting, hence an ensemble algorithm was used.\n",
        "\n",
        "#### Options:\n",
        "A) S1, S2, S3\n",
        "\n",
        "B) S1, S3, S4\n",
        "\n",
        "C) S2, S3, S4\n",
        "\n",
        "D) All of the above\n"
      ],
      "metadata": {
        "id": "N28YS0n9bWSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## SMOTE imbalance\n",
        "\n",
        "\n",
        "#### Context:\n",
        "The Synthetic Minority Over-sampling Technique (SMOTE) is employed to handle class imbalance by generating synthetic samples for minority classes. This technique is crucial for ensuring that machine learning models do not become biased towards the majority class.\n",
        "\n",
        "#### Task:\n",
        "Using the SMOTE technique, determine the resulting sample size for each class after applying the algorithm. You are to predict the uniform size that each class will have post-SMOTE, assuming SMOTE balances all classes to the size of the originally largest class.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Review Initial Class Distribution:** Start by analyzing the initial distribution of your classes. This will help you understand the extent of the imbalance you need to address.\n",
        "2. **Apply SMOTE:** Implement the SMOTE technique on your dataset to balance the class distribution. Use `random_state=42` for reproducibility.\n",
        "3. **Analyze Post-SMOTE Distribution:** After applying SMOTE, examine the new class sizes. All classes should now reflect the size of the largest original class if SMOTE has been applied correctly.\n",
        "4. **Determine Uniform Class Size:** From the balanced dataset, identify the uniform size to which all classes have been adjusted.\n",
        "\n",
        "#### Question:\n",
        "What will be the sample size for each class after applying SMOTE, given that SMOTE adjusts all classes to the size of the largest class present before balancing?\n",
        "\n",
        "#### Options:\n",
        "A) 6540\n",
        "\n",
        "B) 5530\n",
        "\n",
        "C) 7000\n",
        "\n",
        "D) 4210\n",
        "\n"
      ],
      "metadata": {
        "id": "cNJU-HkgdDfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn._____ import ____\n",
        "\n",
        "# TODO: Count class instances before SMOTE from the variable ('y')\n",
        "print(\"\\Before SMOTE:\")\n",
        "_____\n",
        "\n",
        "# TODO: Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote._____(X, __)\n",
        "\n",
        "# TODO: Count class instances after SMOTE on variable 'y_resampled'\n",
        "print(\"\\nAfter SMOTE:\")\n",
        "______"
      ],
      "metadata": {
        "id": "SF2wTOZLDsZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "## Understanding OOB\n",
        "\n",
        "\n",
        "#### Context:\n",
        "In ensemble methods such as Random Forests that utilize bootstrapping, Out-Of-Bag (OOB) data plays a critical role in estimating the model's performance without needing a separate validation set. Understanding what OOB data represents is key to effectively using these ensemble methods in practice.\n",
        "\n",
        "#### Task:\n",
        "Explain the concept of OOB data in the context of ensemble models that use bootstrapping techniques.\n",
        "\n",
        "#### Question:\n",
        "What does OOB data represent in the context of bootstrapping used in ensemble learning methods?\n",
        "\n",
        "#### Statements:\n",
        "S1:  OOB data is the remaining samples which are not selected by bootstrapping.\n",
        "\n",
        "S2:  OOB data is the remaining samples in the validation dataset.\n",
        "\n",
        "S3:  OOB data is the used samples which were selected by bootstrapping.\n",
        "\n",
        "S4:  OOB data is the used samples in the validation dataset.\n"
      ],
      "metadata": {
        "id": "tYGLlQuGjwjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Comparing OOB Scores\n",
        "\n",
        "#### Context:\n",
        "The Out-Of-Bag (OOB) score in Random Forest models offers a unique method of validation by utilizing unsampled data points during the training process as a test set. This task involves comparing the OOB scores of a Random Forest classifier trained on original versus SMOTE-resampled data to evaluate how class balancing affects model performance.\n",
        "\n",
        "#### Task:\n",
        "Implement and compare two Random Forest classifiers: one trained on the original dataset and another on a dataset resampled using SMOTE. Your goal is to analyze the difference in OOB scores between these two approaches.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Balance the Dataset Using SMOTE:** Apply SMOTE to the original dataset to balance class distributions, ensuring that minority classes are adequately represented.\n",
        "2. **Split Both Datasets:** Split both the original and the SMOTE-resampled datasets into training and testing sets using a 33% test size and a random state of 42.\n",
        "3. **Initialize and Train Two Random Forests:** Create two Random Forest classifiers with `oob_score=True` and `random_state=42`. Train one on the original training set and another on the resampled training set.\n",
        "4. **Retrieve and Compare OOB Scores:** After training, extract the OOB scores from both classifiers to measure and compare their estimated performance based on the OOB samples.\n",
        "\n",
        "#### Question:\n",
        "After training Random Forest models on both original and SMOTE-resampled datasets, how do the OOB scores compare, and what does this indicate about the impact of class balancing on model performance?\n",
        "\n",
        "#### Options:\n",
        "A) OOB score is higher for the original dataset, indicating better generalization without resampling.\n",
        "\n",
        "B) OOB score is higher for the SMOTE-resampled dataset, suggesting improved model performance due to better class balance.\n",
        "\n",
        "C) OOB scores are similar, indicating no significant impact from SMOTE resampling on model validation.\n",
        "\n",
        "D) OOB score is lower for the SMOTE-resampled dataset, which may indicate overfitting due to class oversampling.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cgE3IKCYjxvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn._____ import RandomForestClassifier\n",
        "\n",
        "# TODO: Split the original data into training and testing sets\n",
        "X_train_orig, X_test_orig, y_train_orig, y_test_orig = _______(X, ___, test_size=0.33, random_state=42)\n",
        "\n",
        "# TODO: Split the resampled data into training and testing sets\n",
        "X_train_res, X_test_res, y_train_res, y_test_res = ______(_____, y_resampled, test_size=0.33, random_state=42)\n",
        "\n",
        "# TODO: Initialize a Random Forest classifier for the original data\n",
        "rf_classifier_orig = RandomForestClassifier(_____=True, random_state=42) # Hint: Set `oob score` to True\n",
        "\n",
        "# TODO: Train the classifier on the original data\n",
        "rf_classifier_orig.fit(_____, ______)\n",
        "\n",
        "# TODO: Retrieve the OOB score for the original data\n",
        "oob_score_orig = rf_classifier_orig._____ # Hint: Use documentation to access oob score.\n",
        "print(f\"The OOB score for original data is: {oob_score_orig:.4f}\")\n",
        "\n",
        "# Initialize a Random Forest classifier for the resampled data\n",
        "rf_classifier_res = RandomForestClassifier(____=True, random_state=42) # Hint: Set `oob score` to True\n",
        "\n",
        "# TODO: Train the classifier on the resampled data\n",
        "rf_classifier_res.fit(____, _____)\n",
        "\n",
        "# TODO: Retrieve the OOB score for the resampled data\n",
        "oob_score_res = rf_classifier_res.____\n",
        "print(f\"The OOB score for resampled data is: {oob_score_res:.4f}\")\n"
      ],
      "metadata": {
        "id": "QyaXur2egRuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Most Accurate Class\n",
        "\n",
        "#### Context:\n",
        "Random Forest (RF) is a powerful ensemble classifier known for its accuracy and robustness. Understanding which class is most accurately predicted by the RF model can provide insights into the model's strengths and potential biases.\n",
        "\n",
        "#### Task:\n",
        "Use the previously trained rf_classifier, then analyze its predictions to determine which class is predicted most accurately based on recall from the classification report.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Random Forest:** Use the previously trained rf_classifier on SMOTE data.\n",
        "2. **Predict Class Labels:** Use the trained classifier to predict labels for the test set.\n",
        "3. **Generate Classification Report:** Use `classification_report` from `sklearn.metrics` to generate detailed performance metrics for each class.\n",
        "4. **Analyze Recall Values:** Recall for each class indicates the proportion of actual positives that were correctly identified. Identify which class has the highest recall, signifying it as the most accurately predicted class.\n",
        "\n",
        "#### Question:\n",
        "Based on the Random Forest model's predictions, which class is the most accurately predicted in terms of recall?\n",
        "\n",
        "#### Options:\n",
        "A) Pastry\n",
        "\n",
        "B) Z_scratch\n",
        "\n",
        "C) K_scratch\n",
        "\n",
        "D) Stains\n"
      ],
      "metadata": {
        "id": "qZH3gQnH_B6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn._____ import classification_report, confusion_matrix\n",
        "\n",
        "# TODO: Predict the classes for the test set for the resampled data\n",
        "y_pred = rf_classifier_res.______(X_test_res)\n",
        "\n",
        "# Generate the classification report\n",
        "class_report = classification_report(_____, ____, output_dict=True)\n",
        "\n",
        "# Print the classification report for human reading\n",
        "print(classification_report(____, ____))\n",
        "\n",
        "# Find the class with the highest recall (indicating the most correctly classified)\n",
        "most_correctly_classified = max(class_report.keys(), key=lambda x: class_report[x]['recall'] if x != 'accuracy' and isinstance(class_report[x], dict) else 0)\n",
        "most_correctly_classified_score = class_report[most_correctly_classified]['recall']\n",
        "\n",
        "print(f\"The most correctly classified class is: {most_correctly_classified} with a recall of: {most_correctly_classified_score:.4f}\")"
      ],
      "metadata": {
        "id": "lZOLTDYb8QLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Know your RF\n",
        "\n",
        "#### Context:\n",
        "Random Forest is an ensemble method that builds multiple decision trees and combines their predictions to improve accuracy and control overfitting. The placement of features within these trees, especially influential ones like 'Outside_X_index', can significantly affect the model's performance and interpretation.\n",
        "\n",
        "#### Task:\n",
        "Evaluate the statement regarding the placement of a strong predictor within the base learners of a Random Forest. The statement suggests that if 'Outside_X_index' is a very strong predictor, it will be the root node in all the base learners (Decision Trees) of the Random Forest.\n",
        "\n",
        "#### Question:\n",
        "Is the statement \"If 'Outside_X_index' is a very strong predictor of the output variable, then all of the base learners (DT) in a Random Forest will have this feature as the root node\" true or false?\n",
        "\n",
        "#### Options:\n",
        "A) True\n",
        "\n",
        "B) False\n"
      ],
      "metadata": {
        "id": "RWKVEVIgBNED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Optimizing RandomForest\n",
        "\n",
        "#### Context:\n",
        "Randomized Search is an efficient method for hyperparameter tuning that explores a given parameter space to find the optimal settings for a machine learning model. This approach helps enhance model performance by systematically testing different combinations of parameters.\n",
        "\n",
        "#### Task:\n",
        "Implement a Randomized Search CV to optimize a RandomForest Classifier using a specific set of hyperparameters. The goal is to identify the best combination of parameters that leads to the highest cross-validation score.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Set Up Parameter Grid:** Define a parameter grid that includes potential values for 'n estimators', 'max depth', 'min samples split', and 'min samples_leaf'.\n",
        "2. **Initialize RandomForest:** Create a RandomForest classifier ensuring to set `random_state=42` for reproducibility.\n",
        "3. **Configure RandomizedSearchCV:** Set up RandomizedSearchCV with the RandomForest classifier, the defined parameter grid, and specify `n_iter=30` and `cv=2` for 30 iterations with 2-fold cross-validation. Also set `random_state=42`.\n",
        "4. **Execute the Search:** Fit the RandomizedSearchCV on your dataset to find the best parameter settings.\n",
        "5. **Report Results:** Output the best parameters and the best cross-validation score obtained.\n",
        "\n",
        "#### Parameters for Search:\n",
        "```python\n",
        "params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "```\n",
        "\n",
        "#### Question:\n",
        "After performing Randomized Search with the specified parameters on a RandomForest model, what will be the best parameters?\n",
        "\n",
        "#### Options:\n",
        "A) {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 10}\n",
        "\n",
        "B) {'n_estimators': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None}\n",
        "\n",
        "C) {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 10}\n",
        "\n",
        "D) {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 30}\n",
        "\n",
        "#### Note:\n",
        "The parameter grid provided here is intentionally limited due to time constraints and is mainly for practice purposes. To potentially achieve better results, consider expanding the range of parameters and allowing more time for tuning. The current setup is designed to provide a basic understanding of how parameter tuning affects model performance but does not guarantee optimal results."
      ],
      "metadata": {
        "id": "0uD_pbpnBf3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn._____ import RandomizedSearchCV\n",
        "\n",
        "# Define a more extensive parameter grid for Random Search\n",
        "params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "\n",
        "# Initialize the RandomForest classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# TODO: Setup RandomizedSearchCV with randomforest model,  30 iterations and 2-fold cross-validation\n",
        "random_search = RandomizedSearchCV(____=rf, param_distributions=params, ____=30, ___=2, verbose=1, random_state=42)\n",
        "\n",
        "# TODO: Perform the Random Search\n",
        "random_search.___(____, ____)\n",
        "\n",
        "# Print the best parameters and the best score\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "print(\"Best cross-validation score achieved: \", random_search.best_score_)"
      ],
      "metadata": {
        "id": "zSeuTr-qB-_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## RF for Feature Importance\n",
        "\n",
        "#### Context:\n",
        "Random Forest (RF) classifiers not only provide robust predictive capabilities but also offer insights into feature importance within the dataset. This ability to evaluate feature significance is crucial for understanding the factors that most strongly influence the model's predictions.\n",
        "\n",
        "#### Task:\n",
        "Confirm if a RandomForest can be used to derive feature importances and identify the most important feature according to the best-performing model obtained from RandomizedSearchCV.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Retrieve Best Model:** Obtain the best RandomForest model from the RandomizedSearchCV results.\n",
        "2. **Extract Feature Importances:** From the best model, extract the feature importance scores.\n",
        "3. **Identify and Sort Features:** Get the names of the features used in the model and sort them by their importance scores in descending order.\n",
        "4. **Visualize Feature Importances:** Generate a plot to visually display the importance of each feature, emphasizing the feature with the highest score.\n",
        "5. **Determine the Most Important Feature:** Identify and report the feature with the highest importance score.\n",
        "\n",
        "#### Question:\n",
        "Can Feature Importance be effectively applied using a RandomForest classifier? If so, which is the most important feature according to the best RF model from your analysis?\n",
        "\n",
        "#### Options:\n",
        "A) Yes, RF can be used for Feature Importance, and the most important feature is 'Outside-Global-Index'.\n",
        "\n",
        "B) Yes, RF can be used for Feature Importance, and the most important feature is 'Length_of_Conveyer'.\n",
        "\n",
        "C) Yes, RF can be used for Feature Importance, and the most important feature is 'Luminosity-lndex'.\n",
        "\n",
        "D) No, RF cannot be used for Feature Importance.\n",
        "\n"
      ],
      "metadata": {
        "id": "NlAQwPF7gmTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Get the best RandomForest model from RandomizedSearchCV\n",
        "best_rf_model = random_search.____\n",
        "\n",
        "# TODO: Get feature importances from the best model\n",
        "feature_importances = best_rf_model._____\n",
        "\n",
        "# Get feature names\n",
        "feature_names = X_train_res.columns  # Adjust this if your feature names are stored differently\n",
        "\n",
        "# TODO: Sort the features by their importance (and keep track of their names)\n",
        "sorted_indices = np._______(feature_importances)[::-1] # Hint sort only the `indicies` of the feature according to their importances.\n",
        "sorted_importances = feature_importances[sorted_indices]\n",
        "sorted_feature_names = feature_names[sorted_indices]\n",
        "\n",
        "# Print the most important feature and its importance score\n",
        "most_important_feature_name = sorted_feature_names[0]\n",
        "print(f\"The most important feature is: {most_important_feature_name} with an importance score of: {sorted_importances[0]:.4f}\")\n",
        "\n",
        "# TODO: Plot all feature importances sorted\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.barh(range(len(sorted_importances)), ____, align='center', color='skyblue')\n",
        "plt.yticks(range(len(sorted_importances)), sorted_feature_names)\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.ylabel('Feature Names')\n",
        "plt.title('Sorted Feature Importance from the Best RandomForest Model')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to have the highest importance on top\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cmeAJkgkFxrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kt8U1Tp7gprX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}