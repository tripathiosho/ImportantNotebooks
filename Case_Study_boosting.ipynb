{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oL93ODIvRe-D"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tripathiosho/ImportantNotebooks/blob/main/Case_Study_boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing Restaurant Booking Trends with Boosting Algorithms"
      ],
      "metadata": {
        "id": "9euzWZUib2Ko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Context\n",
        "\n",
        "Welcome to the **Scaler Hospitality** Analytics team! Our current initiative aims to enhance the booking management system for a chain of restaurants. Your expertise is requested to analyze the Restaurant Booking dataset, utilizing boosting algorithms to predict booking statuses and understand customer behaviors. This task will support strategic decision-making to optimize booking operations and enhance customer satisfaction.\n",
        "\n",
        "### Dataset Description\n",
        "\n",
        "The dataset you will analyze contains a variety of attributes related to restaurant bookings. These details offer insights into customer preferences, booking patterns, and cancellation trends. Each booking record is characterized by several features:\n",
        "\n",
        "### Booking Attributes:\n",
        "- **id:** Unique identifier for each booking.\n",
        "- **no_of_adults:** Number of adults per booking.\n",
        "- **no_of_children:** Number of children per booking.\n",
        "- **no_of_weekend_nights:** Number of weekend nights booked.\n",
        "- **no_of_week_nights:** Number of weekday nights booked.\n",
        "- **type_of_meal_plan:** Type of meal plan selected (e.g., bed and breakfast, all-inclusive).\n",
        "- **required_car_parking_space:** Indicates if a parking space was required.\n",
        "- **room_type_reserved:** Type of room reserved.\n",
        "- **lead_time:** Number of days between the booking date and the arrival date.\n",
        "- **arrival_year, arrival_month, arrival_date:** Date details for when the booking is scheduled.\n",
        "- **market_segment_type:** The market segment from which the booking originated.\n",
        "- **repeated_guest:** Flag indicating if the guest has booked before.\n",
        "- **no_of_previous_cancellations:** Number of prior cancellations by the guest.\n",
        "- **no_of_previous_bookings_not_canceled:** Number of previous bookings not canceled by the guest.\n",
        "- **avg_price_per_room:** Average price per room type.\n",
        "- **no_of_special_requests:** Number of special requests made by the guest.\n",
        "- **booking_status:** Status of the booking ( confirmed = 0, canceled = 1).\n",
        "\n",
        "Your objective is to use boosting techniques to predict the 'booking_status' and analyze factors that influence booking behaviors. This analysis will help in identifying key trends that could influence policy adjustments, promotional strategies, and operational improvements."
      ],
      "metadata": {
        "id": "Vjt6siwBbpzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/067/435/original/booking.csv\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVem9svbG0wT",
        "outputId": "8c5fbc0e-09ca-4edf-d6bc-c3f263f8ed5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-18 05:32:03--  https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/067/435/original/booking.csv\n",
            "Resolving d2beiqkhq929f0.cloudfront.net (d2beiqkhq929f0.cloudfront.net)... 108.157.172.173, 108.157.172.176, 108.157.172.183, ...\n",
            "Connecting to d2beiqkhq929f0.cloudfront.net (d2beiqkhq929f0.cloudfront.net)|108.157.172.173|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2141217 (2.0M) [text/plain]\n",
            "Saving to: ‘booking.csv’\n",
            "\n",
            "booking.csv         100%[===================>]   2.04M  1.79MB/s    in 1.1s    \n",
            "\n",
            "2024-04-18 05:32:05 (1.79 MB/s) - ‘booking.csv’ saved [2141217/2141217]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"booking.csv\")\n",
        "\n",
        "df_original = df.copy()\n",
        "\n",
        "Target = 'booking_status'\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "0zOUQ2li4JFl",
        "outputId": "f88af595-ff58-4bf8-d6ed-8e35799b6f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id  no_of_adults  no_of_children  no_of_weekend_nights  no_of_week_nights  \\\n",
              "0   0             2               0                     0                  2   \n",
              "1   1             2               0                     1                  2   \n",
              "2   2             2               0                     0                  1   \n",
              "3   3             1               0                     0                  2   \n",
              "4   4             2               0                     1                  0   \n",
              "\n",
              "   type_of_meal_plan  required_car_parking_space  room_type_reserved  \\\n",
              "0                  1                           0                   0   \n",
              "1                  0                           0                   0   \n",
              "2                  0                           0                   0   \n",
              "3                  1                           0                   0   \n",
              "4                  0                           0                   0   \n",
              "\n",
              "   lead_time  arrival_year  arrival_month  arrival_date  market_segment_type  \\\n",
              "0          9          2018              1            14                    1   \n",
              "1        117          2018              7            29                    0   \n",
              "2        315          2018             12             2                    0   \n",
              "3         32          2018             12             1                    1   \n",
              "4        258          2018             10            16                    0   \n",
              "\n",
              "   repeated_guest  no_of_previous_cancellations  \\\n",
              "0               1                            11   \n",
              "1               0                             0   \n",
              "2               0                             0   \n",
              "3               0                             0   \n",
              "4               0                             0   \n",
              "\n",
              "   no_of_previous_bookings_not_canceled  avg_price_per_room  \\\n",
              "0                                     0               67.50   \n",
              "1                                     0               72.25   \n",
              "2                                     0               52.00   \n",
              "3                                     0               56.00   \n",
              "4                                     0              100.00   \n",
              "\n",
              "   no_of_special_requests  booking_status  \n",
              "0                       0               0  \n",
              "1                       0               0  \n",
              "2                       0               0  \n",
              "3                       0               0  \n",
              "4                       0               1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-47e4e04b-1a22-4fb5-9551-a1d7ec6f39c1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>no_of_adults</th>\n",
              "      <th>no_of_children</th>\n",
              "      <th>no_of_weekend_nights</th>\n",
              "      <th>no_of_week_nights</th>\n",
              "      <th>type_of_meal_plan</th>\n",
              "      <th>required_car_parking_space</th>\n",
              "      <th>room_type_reserved</th>\n",
              "      <th>lead_time</th>\n",
              "      <th>arrival_year</th>\n",
              "      <th>arrival_month</th>\n",
              "      <th>arrival_date</th>\n",
              "      <th>market_segment_type</th>\n",
              "      <th>repeated_guest</th>\n",
              "      <th>no_of_previous_cancellations</th>\n",
              "      <th>no_of_previous_bookings_not_canceled</th>\n",
              "      <th>avg_price_per_room</th>\n",
              "      <th>no_of_special_requests</th>\n",
              "      <th>booking_status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>67.50</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>117</td>\n",
              "      <td>2018</td>\n",
              "      <td>7</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>72.25</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>315</td>\n",
              "      <td>2018</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>52.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>2018</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>56.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>258</td>\n",
              "      <td>2018</td>\n",
              "      <td>10</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47e4e04b-1a22-4fb5-9551-a1d7ec6f39c1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-47e4e04b-1a22-4fb5-9551-a1d7ec6f39c1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-47e4e04b-1a22-4fb5-9551-a1d7ec6f39c1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ce8fec01-cada-43af-975b-04b3d0242b7d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ce8fec01-cada-43af-975b-04b3d0242b7d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ce8fec01-cada-43af-975b-04b3d0242b7d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 42100,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12153,\n        \"min\": 0,\n        \"max\": 42099,\n        \"num_unique_values\": 42100,\n        \"samples\": [\n          8768,\n          38340,\n          7104\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"no_of_adults\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          4,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"no_of_children\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 9,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0,\n          2,\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"no_of_weekend_nights\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 7,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1,\n          6,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"no_of_week_nights\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 17,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          2,\n          1,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type_of_meal_plan\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"required_car_parking_space\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"room_type_reserved\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lead_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 81,\n        \"min\": 0,\n        \"max\": 443,\n        \"num_unique_values\": 338,\n        \"samples\": [\n          67,\n          311\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"arrival_year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2017,\n        \"max\": 2018,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2017,\n          2018\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"arrival_month\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 12,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          3,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"arrival_date\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1,\n        \"max\": 31,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          23,\n          25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"market_segment_type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"repeated_guest\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"no_of_previous_cancellations\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 13,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          6,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"no_of_previous_bookings_not_canceled\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 58,\n        \"num_unique_values\": 42,\n        \"samples\": [\n          24,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_price_per_room\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37.13916508527347,\n        \"min\": 0.0,\n        \"max\": 540.0,\n        \"num_unique_values\": 2286,\n        \"samples\": [\n          141.5,\n          66.45\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"no_of_special_requests\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"booking_status\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HL_nZNuOdRLR",
        "outputId": "4c09fdf8-20b3-4e8a-b7ea-b004169a7ef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'no_of_adults', 'no_of_children', 'no_of_weekend_nights',\n",
              "       'no_of_week_nights', 'type_of_meal_plan', 'required_car_parking_space',\n",
              "       'room_type_reserved', 'lead_time', 'arrival_year', 'arrival_month',\n",
              "       'arrival_date', 'market_segment_type', 'repeated_guest',\n",
              "       'no_of_previous_cancellations', 'no_of_previous_bookings_not_canceled',\n",
              "       'avg_price_per_room', 'no_of_special_requests', 'booking_status'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_original.drop(Target, axis = 1)\n",
        "y = df_original[Target]\n",
        "\n",
        "df_original[Target].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxdhibBhPJZh",
        "outputId": "c104b129-3d41-4c11-82e1-dcb1139b1932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "booking_status\n",
              "0    25596\n",
              "1    16504\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Assesments for Boosting 1"
      ],
      "metadata": {
        "id": "oL93ODIvRe-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing Restaurant Booking Patterns\n",
        "\n",
        "#### Context:\n",
        "Welcome to the **Scaler Hospitality** Analytics team! We're diving into the Restaurant Booking dataset to enhance our booking management system. Your insights will help us optimize operations and improve customer experiences.\n",
        "\n",
        "#### Task:\n",
        "Your initial task is to conduct an exploratory data analysis to uncover insights about unusual booking patterns, specifically, bookings where neither adults nor children are included.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Data Inspection:** Examine the dataset for entries that indicate bookings with zero adults and zero children.\n",
        "2. **Count Such Bookings:** Calculate the number of such instances to understand the prevalence of this anomaly.\n",
        "\n",
        "#### Question:\n",
        "Based on the dataset provided, how many bookings are recorded where both the number of adults and the number of children are zero?\n",
        "\n",
        "#### Options:\n",
        "A) 147 bookings\n",
        "\n",
        "B) 16 bookings\n",
        "\n",
        "C) 2 bookings\n",
        "\n",
        "D) 0 bookings\n",
        "\n"
      ],
      "metadata": {
        "id": "johEey6tRiC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Examine the dataset for entries with zero adults and zero children\n",
        "\n",
        "booking_counts = ____ # Hint: Use 'no_of_adults' and 'no_of_children' column to extract this information\n",
        "print(\"Number of bookings with zero adults and zero children:\", booking_counts)"
      ],
      "metadata": {
        "id": "2EwMwdYpS3HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Booking Status Distribution by Month\n",
        "\n",
        "#### Context:\n",
        "Understanding seasonal trends in booking cancellations can help the Scaler Hospitality Analytics team develop targeted strategies to minimize losses and improve customer retention. You are tasked with visualizing how booking statuses vary by month.\n",
        "\n",
        "#### Task:\n",
        "Create a visual representation to analyze the distribution of booking cancellations across different months using the 'arrival_month' and 'booking_status' columns in the dataset.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Group and Count Data:** Use the dataset to group entries by 'arrival_month' and 'booking_status'. Count the occurrences of each booking status (canceled or not canceled) for every month.\n",
        "2. **Visualize Data:** Generate a bar chart that displays the counts of canceled and not canceled bookings for each month. This will help in visually comparing the booking trends over the year.\n",
        "3. **Analyze Trends:** Examine the bar chart to determine which month has the highest number of cancellations and which has the lowest.\n",
        "\n",
        "#### Question:\n",
        "After analyzing the bar chart representing the booking status count for each month, identify the months with the highest and lowest number of booking cancellations.\n",
        "\n",
        "#### Options:\n",
        "A) Highest: August, Lowest: January\n",
        "\n",
        "B) Highest: December, Lowest: February\n",
        "\n",
        "C) Highest: July, Lowest: November\n",
        "\n",
        "D) Highest: May, Lowest: March"
      ],
      "metadata": {
        "id": "CBi7ICegcTiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# TODO: Create a pivot table to count booking status for each month\n",
        "booking_status_counts = _____ # Hint: Use 'arrival_month' and 'booking_status' columns and convert into pivot.\n",
        "\n",
        "print(booking_status_counts)\n",
        "\n",
        "# Plotting using the pivot table\n",
        "booking_status_counts.plot(kind=____, figsize=(10, 7)) # Hint: use bar plot\n",
        "plt.title('Booking Status Count by Month')\n",
        "plt.xlabel('Arrival Month')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Booking Status')\n",
        "plt.tight_layout()  # Adjust layout to not cut off labels\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jka1vOMqsIfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Date Engineering\n",
        "\n",
        "#### Context:\n",
        "Effective date engineering can uncover deeper insights in time-series data such as hotel bookings. This task involves transforming basic date-related columns into a comprehensive set of datetime features that can be used for advanced analysis and model building.\n",
        "\n",
        "#### Task:\n",
        "Develop a Python function named `process_arrival_date` that converts basic date fields in a hotel booking dataset into a rich set of datetime features. This function should handle data transformation and cleanup efficiently.\n",
        "\n",
        "#### Function Requirements:\n",
        "1. **Input and Output:** The function should accept a DataFrame and return a modified DataFrame with new datetime-related features.\n",
        "2. **Rename Columns:** Change 'arrival_year', 'arrival_month', and 'arrival_date' to 'year', 'month', and 'day'.\n",
        "3. **Create Datetime Column:** Combine 'year', 'month', and 'day' into a new 'date' column as a datetime object. Use `pd.to_datetime` with `errors='coerce'` to handle any data inconsistencies.\n",
        "4. **Extract Datetime Features:** From the 'date' column, derive and add new columns such as 'year', 'month', 'week', 'day', 'dayofweek', 'quarter', and 'dayofyear'.\n",
        "5. **Cleanup:** Remove any intermediate columns like the temporary 'date' column after extracting necessary features.\n",
        "\n",
        "#### Instructions:\n",
        "- **Define the Function:** Implement the `process_arrival_date` function based on the above requirements.\n",
        "- **Apply the Function:** Test the functionality of your function by applying it to a DataFrame `df`.\n",
        "- **Verify Output:** Check the resulting DataFrame to ensure it includes all new date-related features and excludes any redundant or temporary columns.\n",
        "\n",
        "#### Question:\n",
        "After applying the `process_arrival_date` function to a DataFrame and transforming the date-related columns, what are the number of missing values (NaN) in the newly created columns?\n",
        "\n",
        "#### Options:\n",
        "A) 0\n",
        "\n",
        "B) 50\n",
        "\n",
        "C) 100\n",
        "\n",
        "D) 43\n",
        "\n"
      ],
      "metadata": {
        "id": "K8wkUfbHcXlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "def process_arrival_date(data):\n",
        "    # Remove any pre-existing date columns that could conflict with the new transformations\n",
        "    data.drop(columns=['year', 'month', 'day'], inplace=True, errors='ignore')\n",
        "\n",
        "    # Renaming columns for uniformity\n",
        "    temp = data.rename(columns={\n",
        "        'arrival_year': 'year',\n",
        "        'arrival_month': 'month',\n",
        "        'arrival_date': 'day'\n",
        "    })\n",
        "\n",
        "    # TODO: Creating a datetime column from the year, month, and day columns\n",
        "    data['date'] = pd._____(temp[['year', 'month', 'day']], errors='coerce')\n",
        "\n",
        "    # TODO: Extract date features\n",
        "    data['year'] = data['date'].dt.____\n",
        "    data['month'] = data['date'].dt.____\n",
        "    data['week'] = data['date'].dt.isocalendar().week.astype(float)\n",
        "    data['day'] = data['date'].dt.____\n",
        "    data['dayofweek'] = data['date'].dt.____\n",
        "    data['quarter'] = data['date'].dt.____\n",
        "    data['dayofyear'] = data['date'].dt.____\n",
        "\n",
        "    # Cleanup: Remove 'date' column after extracting necessary features\n",
        "    data.drop(columns=____, inplace=True)\n",
        "    return data\n",
        "\n",
        "# Test the function by applying it to a DataFrame 'df'\n",
        "new_processed_df = process_arrival_date(df)\n",
        "\n",
        "# Display missing values in the newly created columns to check for any NaN entries\n",
        "missing_values = new_processed_df.isna().sum()\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "POX8ZWugucoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Sequencing Steps in Boosting Algorithm\n",
        "\n",
        "#### Context:\n",
        "Boosting is a powerful ensemble machine learning technique that combines multiple weak learners to form a strong predictive model. Each learner is sequentially added, specifically focusing on the errors of the previous models.\n",
        "\n",
        "#### Task:\n",
        "Organize the steps involved in the implementation of a boosting algorithm. This will test your understanding of the sequential nature of how boosting models are constructed and optimized.\n",
        "\n",
        "#### Question\n",
        "Consider the key points listed about the boosting algorithm process. Put the following steps in the correct order, from first to last, as they would occur in the implementation of a boosting algorithm. <br\n",
        "\n",
        "#### Steps to Sequence:\n",
        "<ol>\n",
        "<li>Sequential Model Development</li>\n",
        "<li>Base Learner Initialization</li>\n",
        "<li>Initiation with a Preliminary Model</li>\n",
        "<li>Combination through Addition</li>\n",
        "<li>Prediction Weight Adjustment</li>\n",
        "</ol>\n",
        "\n",
        "#### Options\n",
        "A] 2, 3, 1, 4, 5\n",
        "\n",
        "B] 3, 2, 1, 5, 4\n",
        "\n",
        "C] 2, 3, 4, 1, 5\n",
        "\n",
        "D] 3, 1, 2, 5, 4\n",
        "\n"
      ],
      "metadata": {
        "id": "ZoEmNoit9IvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Simple Boosting Algorithm\n",
        "\n",
        "#### Context:\n",
        "Boosting is an ensemble technique that combines multiple weak learners to create a strong classifier. Each weak learner is trained sequentially, with each one focusing on the errors made by the previous models.\n",
        "\n",
        "#### Task:\n",
        "Complete the Python code provided to implement a simple boosting algorithm using two Decision Tree classifiers. Then, determine the accuracy of the combined model.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Fill in the Blanks:** Complete the Python code where indicated (blanks marked with `__1__`, `__2__`, `__3__`, and `__4__`) to ensure the boosting algorithm functions correctly.\n",
        "2. **Run the Code:** Execute the completed code on your dataset to train the Decision Trees and combine their predictions.\n",
        "3. **Verify and Analyze Results:** Calculate the accuracy of the final combined prediction and select the correct answer based on the output.\n",
        "\n",
        "Note: Target = 'booking_status', y = df[Target] and X = df.drop(Target, axis = 1)\n",
        "\n",
        "#### Question:\n",
        "After implementing the described boosting process and running the complete code, what is the accuracy of the final combined predictions?\n",
        "\n",
        "#### Options:\n",
        "A) Above 90%\n",
        "\n",
        "B) Between 80% and 90%\n",
        "\n",
        "C) Between 70% and 80%\n",
        "\n",
        "D) Below 70%\n"
      ],
      "metadata": {
        "id": "j6_bK5yJPyBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize weights\n",
        "weights = np.ones(len(X)) / len(X)\n",
        "\n",
        "# Train first weak learner\n",
        "dt1 = DecisionTreeClassifier(random_state=10, max_depth=1)\n",
        "dt1.fit(X, y)\n",
        "y_pred_1 = dt1.predict(X)\n",
        "\n",
        "# Update weights function\n",
        "def update_weights(y, y_pred, weights):\n",
        "    for i in range(len(weights)):\n",
        "        if y[i] __1__ y_pred[i]: # Hint: If actual value is not same as predicted value.\n",
        "            weights[i] *= 1.5  # Increase the weight for misclassified instances\n",
        "        else:\n",
        "            weights[i] *= 0.5  # Decrease the weight for correctly classified instances\n",
        "    return weights / np.sum(weights)  # Normalize the weights\n",
        "\n",
        "# Apply first learner and update weights\n",
        "weights = update_weights(y, __2__, weights) # Hint: update weights using actual and predicted.\n",
        "\n",
        "# Train second weak learner\n",
        "dt2 = DecisionTreeClassifier(random_state=10, max_depth=1)\n",
        "dt2.fit(X, y, sample_weight= __3__)   # Utilize updated weights for learning for this sample_weight is used.\n",
        "y_pred_2 = dt2.predict(X)\n",
        "\n",
        "# Combine the weak learners' predictions\n",
        "# Assuming a simple average for combination of both the predictions\n",
        "final_prediction = np.round((y_pred_1 + __4__) / 2.0).astype(int)\n",
        "\n",
        "# Calculate the accuracy\n",
        "final_accuracy = accuracy_score(y, final_prediction)\n",
        "print(f\"Final combined prediction accuracy: {final_accuracy}\")"
      ],
      "metadata": {
        "id": "Oz89FWSgD94K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "## Boosting Steps residual\n",
        "\n",
        "#### Context:\n",
        "Gradient Boosting is an advanced boosting technique that builds models sequentially, each new model correcting errors made by the previous ones. A key component of Gradient Boosting involves the use of pseudo residuals to guide the learning of subsequent models.\n",
        "\n",
        "#### Task:\n",
        "Organize the steps involved in using pseudo residuals within a Gradient Boosting framework. This task will test your understanding of the methodical process through which boosting models iteratively enhance their accuracy.\n",
        "\n",
        "#### Steps:\n",
        "\n",
        "1. Train a new learner on the pseudo residuals.\n",
        "2. Calculate pseudo residuals based on the loss function's derivative.\n",
        "3. Initialize the model with a simple estimator (e.g., the mean of the target variable).\n",
        "4. Update the model by adding the new learner, scaled by the learning rate.\n",
        "5. Evaluate the current model and calculate the loss for each instance.\n",
        "\n",
        "#### Question\n",
        "\n",
        "Arrange the following steps to reflect how pseudo residuals are used in the iterative process of boosting.\n",
        "\n",
        "\n",
        "Options:\n",
        "\n",
        "A. 3, 5, 2, 1, 4\n",
        "\n",
        "B. 5, 2, 1, 3, 4\n",
        "\n",
        "C. 2, 3, 1, 4, 5\n",
        "\n",
        "D. 3, 1, 5, 2, 4\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VyZBicD4hps7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Code Boosting resedual\n",
        "\n",
        "#### Context:\n",
        "Gradient Boosting is a sequential learning technique in regression and classification that builds models progressively, using an ensemble of weak prediction models like decision trees. It focuses on minimizing errors primarily through the use of pseudo residuals.\n",
        "\n",
        "#### Task:\n",
        "Complete the Python code provided below to implement a simple gradient boosting algorithm using pseudo residuals. You will fill in the missing parts of the code, run it, and determine the model's accuracy using mean squared error.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Fill in the Blanks:** Complete the Python code where indicated (blanks marked with `__1__`, `__2__`, `__3__`, and `__4__`) to ensure the boosting algorithm functions correctly.\n",
        "2. **Run the Code:** Execute the completed code on your dataset to train the Decision Tree regressors and update the model predictions.\n",
        "3. **Verify and Analyze Results:** Calculate the final model accuracy using the mean squared error and select the correct accuracy from the given options.\n",
        "\n",
        "\n",
        "#### Question:\n",
        "After filling in the blanks and executing the code, what is the final model accuracy based on the mean squared error?\n",
        "\n",
        "Options:\n",
        "\n",
        "A. Above 90%\n",
        "\n",
        "B. Between 80% and 90%\n",
        "\n",
        "C. Between 70% and 80%\n",
        "\n",
        "D. Below 70%\n"
      ],
      "metadata": {
        "id": "osbMOYr7QHJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "y_pred = np.full(shape=y.shape, fill_value=np.mean(y))\n",
        "\n",
        "# Initialize pseudo residuals\n",
        "pseudo_residuals = y - __1__\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Train first weak learner on pseudo residuals\n",
        "dt1 = DecisionTreeRegressor(max_depth=1, random_state=10)\n",
        "dt1.fit(X, __2__)\n",
        "y_pred_1 = dt1.predict(X)\n",
        "\n",
        "# Update model predictions\n",
        "y_pred += y_pred_1 * __3__ # Hint: use learning rate.\n",
        "\n",
        "# Calculate new pseudo residuals\n",
        "pseudo_residuals = y - y_pred\n",
        "\n",
        "# Train second weak learner on new pseudo residuals\n",
        "dt2 = DecisionTreeRegressor(max_depth=1, random_state=10)\n",
        "dt2.fit(X, pseudo_residuals)\n",
        "y_pred_2 = dt2.predict(X)\n",
        "\n",
        "# Update model predictions\n",
        "y_pred += __4__* learning_rate\n",
        "\n",
        "# Calculate the accuracy using the mean squared error as a measure of accuracy for regression\n",
        "final_accuracy = 1 - mean_squared_error(y, y_pred)\n",
        "print(f\"Final model accuracy: {final_accuracy}\")"
      ],
      "metadata": {
        "id": "_QQWaXhRKl-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Comparing Gradient Boosting Performance\n",
        "\n",
        "\n",
        "#### Context:\n",
        "Effectively preprocessing data can significantly impact the performance of machine learning models. This task involves evaluating how a Gradient Boosting Classifier performs on both an original dataset and the same dataset after preprocessing and missing value removal.\n",
        "\n",
        "#### Task:\n",
        "Implement a Gradient Boosting Classifier and assess its accuracy on both the original and a preprocessed version of a dataset. This will help determine the impact of preprocessing on model performance.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Split Original Data:** Use `train_test_split` to divide the original dataset (`df`) into training and test sets, setting `test_size` to 0.2 and `random_state` to 10.\n",
        "2. **Train on Original Data:** Fit a `GradientBoostingClassifier` on the original training data and evaluate its accuracy on the original test set.\n",
        "3. **Preprocess the Dataset:** Apply the `process_arrival_date` function and remove rows with missing values to create a cleaned dataset (`new_processed_df`).\n",
        "4. **Split Preprocessed Data:** Split the preprocessed dataset (`new_processed_df`) into training and test sets, again with a `test_size` of 0.2 and `random_state` to 10.\n",
        "5. **Train on Preprocessed Data:** Train a new `GradientBoostingClassifier` on the preprocessed training data and evaluate its accuracy on the preprocessed test set.\n",
        "6. **Compare Accuracies:** Assess whether the model's accuracy improved, declined, or remained the same after preprocessing.\n",
        "\n",
        "#### Question:\n",
        "After implementing the Gradient Boosting Classifier and comparing its performance on both the original and the preprocessed datasets, which statement accurately describes the change in accuracy?\n",
        "\n",
        "#### Options:\n",
        "A) The accuracy is higher on the original dataset.\n",
        "\n",
        "B) The accuracy is higher on the preprocessed dataset.\n",
        "\n",
        "C) The accuracy remains the same on both datasets.\n",
        "\n",
        "D) The accuracy comparison cannot be determined without further information.\n"
      ],
      "metadata": {
        "id": "BrlpYIw1L6Bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn._____ import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# TODO: Split original data\n",
        "X_train_orig, X_test_orig, y_train_orig, y_test_orig = _____(X, y, test_size=0.2, random_state=10)\n",
        "\n",
        "# TODO: Train Gradient Boosting Classifier on original data\n",
        "gbc_orig = _____(random_state=10)\n",
        "gbc_orig.____(X_train_orig, y_train_orig)\n",
        "pred_orig = gbc_orig.____(X_test_orig)\n",
        "accuracy_orig = accuracy_score(y_test_orig, pred_orig)\n",
        "\n",
        "# Using the Processed dataset from the previous process_arrival_date\n",
        "new_df = new_processed_df.dropna()\n",
        "\n",
        "# TODO: Split processed data (ensure to select the same target variable 'y')\n",
        "X_train_proc, X_test_proc, y_train_proc, y_test_proc = train_test_split(new_processed_df.drop(Target, axis = 1), new_processed_df[____], test_size=0.2, random_state=10)\n",
        "\n",
        "# TODO: Train Random Forest on processed data\n",
        "gbc_proc = GradientBoostingClassifier(random_state=10)\n",
        "gbc_proc.fit(_____, y_train_proc)\n",
        "pred_proc = gbc_proc.predict(_____)\n",
        "accuracy_proc = accuracy_score(y_test_proc, pred_proc)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Accuracy on original data: {accuracy_orig}\")\n",
        "print(f\"Accuracy on processed data: {accuracy_proc}\")"
      ],
      "metadata": {
        "id": "fUu0gbTzw5G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Boosting-2"
      ],
      "metadata": {
        "id": "Q78hzKJmL2Qc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "### GBD Feature Importance\n",
        "\n",
        "#### Context:\n",
        "Feature importance is a vital concept in machine learning, allowing data scientists to understand better which features contribute most to a model's predictions. Using a Gradient Boosting Classifier, this task focuses on determining the most influential feature in a dataset.\n",
        "\n",
        "#### Task:\n",
        "Train a Gradient Boosting Classifier on a given dataset and analyze the model to identify which feature is considered the most important based on the trained model.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Split the Data:** Use `train_test_split` to divide the dataset into training and test sets with a `test_size` of 0.2 and `random_state` of 10.\n",
        "2. **Train the Model:** Fit a `GradientBoostingClassifier` with default parameters on the training data. Ensure `random_state` is set to 10 for reproducibility.\n",
        "3. **Extract Feature Importance:** After training, use the model's `feature_importances_` attribute to determine the significance of each feature.\n",
        "4. **Identify the Most Important Feature:** Determine which feature has the highest importance score.\n",
        "\n",
        "#### Question:\n",
        "After training the Gradient Boosting Classifier and examining the feature importance scores, which feature is considered most important?\n",
        "\n",
        "#### Options:\n",
        "A) 'no_of_special_requests'\n",
        "\n",
        "B) 'market_segment_type'\n",
        "\n",
        "C) 'lead_time'\n",
        "\n",
        "D) 'avg_price_per_room'\n"
      ],
      "metadata": {
        "id": "IQlBJONm3k0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = _____(X, y, test_size=0.2, random_state=10)\n",
        "\n",
        "# Initializing and training the Gradient Boosting Classifier\n",
        "clf = GradientBoostingClassifier(random_state=10)\n",
        "clf.____(_____, _____)         # Hint: Fit the clasifier with training data\n",
        "\n",
        "# TODO: Extracting feature importances\n",
        "feature_importances = ______\n",
        "\n",
        "# Feature names\n",
        "features = np.array(X.columns)\n",
        "\n",
        "# TODO: Identifying the most important feature\n",
        "most_important_feature = _______\n",
        "\n",
        "print(f\"The most important feature is: {most_important_feature}\")\n",
        "\n",
        "sns.barplot(y= features, x=feature_importances)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dbKolGJ0zj-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Optimizing Learning Rate\n",
        "\n",
        "#### Context:\n",
        "The learning rate in gradient boosting models is a crucial parameter that scales the contribution of each tree. It can significantly affect model performance by controlling how quickly the model adapts to the complex underlying patterns in the data.\n",
        "\n",
        "#### Task:\n",
        "Your task is to find the optimal learning rate for a Gradient Boosting Classifier applied to the Reservation Cancellation Dataset. This involves comparing the performance of models trained with different learning rates.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Split the Data:** Use `train_test_split` to divide the dataset into training and test sets, setting `test_size` to 0.2 and `random_state` to 10.\n",
        "2. **Define Learning Rates:** Test the classifier's performance across a range of learning rates: {0.1, 0.05, 0.01, 0.005, 0.001}.\n",
        "3. **Train and Evaluate Models:** For each learning rate, train a `GradientBoostingClassifier` and evaluate its accuracy on the test set.\n",
        "4. **Determine Optimal Learning Rate:** Identify the learning rate that results in the highest accuracy.\n",
        "\n",
        "#### Question:\n",
        "After evaluating Gradient Boosting Classifier models with different learning rates on the Reservation Cancellation Dataset, which learning rate yielded the highest accuracy on the test set?\n",
        "\n",
        "#### Options:\n",
        "A) 0.01\n",
        "\n",
        "B) 0.1\n",
        "\n",
        "C) 0.05\n",
        "\n",
        "D) 0.001\n",
        "\n",
        "E) 0.005\n",
        "\n"
      ],
      "metadata": {
        "id": "qBrnLJiB8i2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the learning rates to test\n",
        "learning_rates = [0.1, 0.05, 0.01, 0.005, 0.001]\n",
        "best_accuracy = 0\n",
        "best_learning_rate = 0\n",
        "\n",
        "# TODO: Train a model for each learning rate and evaluate its accuracy\n",
        "\n",
        "for lr in _____: # Hint: iterate over all learning rates\n",
        "    model = GradientBoostingClassifier(_____=lr, random_state=10)\n",
        "    model.fit(____, y_train)\n",
        "    accuracy = accuracy_score(y_test, model.____(X_test))\n",
        "    print(lr, \" : \", accuracy)\n",
        "\n",
        "    if accuracy ___ best_accuracy:  # Hint: add the necessary condition\n",
        "        best_accuracy = ____\n",
        "        best_learning_rate = lr\n",
        "\n",
        "print(f\"The best learning rate is: {best_learning_rate} with an accuracy of: {best_accuracy}\")"
      ],
      "metadata": {
        "id": "cbFlrT120OOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Boosting Max Depth\n",
        "\n",
        "#### Context:\n",
        "The `max_depth` parameter in Gradient Boosting Classifier determines the maximum depth of the individual trees. Adjusting this parameter affects the model's complexity and its ability to capture the underlying patterns in the data, thus influencing performance.\n",
        "\n",
        "#### Task:\n",
        "Your task is to identify the optimal `max_depth` value that results in the highest accuracy for a Gradient Boosting Classifier applied to a dataset. This involves training and evaluating the classifier at various tree depths.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Prepare Data:** Split the dataset into training and test sets using `train_test_split`, with a `test_size` of 0.2 and `random_state` of 10. Separate the dataset into `X` (features) and `y` (target).\n",
        "2. **Define Depth Range:** Test the classifier's performance across a range of depths from 1 to 15.\n",
        "3. **Train and Evaluate Models:** For each depth value, train a `GradientBoostingClassifier` and evaluate its accuracy on the test set.\n",
        "4. **Determine Optimal Depth:** Identify the depth that results in the highest accuracy.\n",
        "\n",
        "#### Question:\n",
        "After training the Gradient Boosting Classifier at different tree depths and evaluating the models, which `max_depth` resulted in the highest accuracy on the test set?\n",
        "\n",
        "#### Options:\n",
        "A) 6\n",
        "\n",
        "B) 7\n",
        "\n",
        "C) 8\n",
        "\n",
        "D) 10\n",
        "\n",
        "Note: Remember to maintain the random_state = 10 in train_test_split and  GradientBoostingClassifier to ensure consistent results. Separate the dataset into X and y, with X comprising all features excluding the target, and y being the target itself.\n"
      ],
      "metadata": {
        "id": "sFYTlAU1FKJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = []\n",
        "best_depth = 0\n",
        "best_acc = 0\n",
        "\n",
        "# TODO: Train a model for each depth and evaluate its accuracy\n",
        "for d in range(1,15):\n",
        "    model = GradientBoostingClassifier(____= d , random_state=10)\n",
        "    model.____(X_train, ____)\n",
        "    accuracy = accuracy_score(y_test, ___.predict(X_test))\n",
        "    acc.append(accuracy)\n",
        "\n",
        "    if accuracy ___ best_acc:   # Hint: add the necessary condition\n",
        "      best_acc = accuracy\n",
        "      best_depth = ___\n",
        "\n",
        "\n",
        "# Create the line plot\n",
        "sns.lineplot(x=range(1, 15), y=acc)\n",
        "\n",
        "# Remove x-ticks\n",
        "plt.xticks([])\n",
        "\n",
        "# Add x-axis label\n",
        "plt.title(\"Find the max_depth\")\n",
        "plt.xlabel('max_depth')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "# Annotate the line at x = best_depth\n",
        "highest_acc = acc[best_depth - 1]  # Adjusting for zero-based indexing\n",
        "plt.axvline(x=best_depth, color='gray', linestyle='--')\n",
        "plt.text(best_depth + 0.2, highest_acc, f\"depth = {best_depth}\", verticalalignment='center')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DLPhMQer0iRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "### Impact of Feature Scaling\n",
        "\n",
        "#### Context:\n",
        "Feature scaling can significantly influence the performance of many machine learning algorithms, particularly those that are distance-based. However, decision trees and their ensembles (like Gradient Boosting) typically do not require feature scaling to perform effectively since they are not sensitive to the scale of the data.\n",
        "\n",
        "#### Task:\n",
        "Investigate whether scaling affects the performance of a Gradient Boosting Classifier by training two models: one with scaled features and one with original features.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Prepare Data:** Split the dataset into `X` (features) and `y` (target). Then use `train_test_split` to divide the data into training and test sets, setting `test_size` to 0.2 and `random_state` to 10.\n",
        "2. **Scale Features:** Apply `StandardScaler` to scale `X_train` and `X_test` for the first model.\n",
        "3. **Train Models:** Train two Gradient Boosting Classifiers—one on the scaled training data and the other on the original training data. Ensure both models use `random_state=10`.\n",
        "4. **Evaluate Models:** Calculate the accuracy of both models on their respective test sets.\n",
        "5. **Compare Accuracies:** Calculate the difference in accuracy between the scaled and the unscaled model.\n",
        "\n",
        "#### Question:\n",
        "After training and evaluating both models, what will be the output of `accuracy_with_scaled_data - accuracy_without_scaled_data`?\n",
        "\n",
        "#### Options:\n",
        "A) The difference will be positive, indicating better performance with scaled features.\n",
        "\n",
        "B) The difference will be negative, indicating better performance without scaling.\n",
        "\n",
        "C) The difference will be zero, indicating no impact of scaling on performance.\n",
        "\n",
        "D) It is impossible to determine without additional information about the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "teKYOh85I_1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.____ import StandardScaler\n",
        "\n",
        "# Assuming X_train and X_test are already defined\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# TODO: Fit on training data and transform it\n",
        "X_train_scaled = scaler.____(X_train)\n",
        "\n",
        "# TODO: Transform the test data\n",
        "X_test_scaled = scaler.____(X_test)\n",
        "\n",
        "# TODO: Train and test the GBC on the Scaled data\n",
        "model_with_scaled = GradientBoostingClassifier(random_state=10)\n",
        "model_with_scaled.fit(_____, y_train)\n",
        "accuracy_with_scaled = accuracy_score(y_test, model_with_scaled.predict(____))\n",
        "\n",
        "# TODO: Train and test the GBC on the Original data (unscaled)\n",
        "model_without_scaled = GradientBoostingClassifier(random_state=10)\n",
        "model_without_scaled.fit(X_train, ____)\n",
        "accuracy_without_scaled = accuracy_score(y_test, model_without_scaled.predict(____))\n",
        "\n",
        "print(f\"accuracy_with_scaled = {accuracy_with_scaled}\")\n",
        "print(f\"accuracy_without_scaled = {accuracy_without_scaled}\")\n",
        "\n",
        "print(f\"Difference in accuracy = {____ - ____}\") # TODO:"
      ],
      "metadata": {
        "id": "XzWvm47U3oR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Hyperparameter Tuning with Grid Search\n",
        "\n",
        "#### Context:\n",
        "Hyperparameter tuning is a critical step in optimizing machine learning models. Using GridSearchCV, you will tune the hyperparameters of a Gradient Boosting Classifier to find the combination that achieves the best performance on a given dataset.\n",
        "\n",
        "#### Task:\n",
        "Apply GridSearchCV to optimize a Gradient Boosting Classifier's hyperparameters on a split dataset. Determine the set of hyperparameters that results in the highest cross-validation performance.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Split the Data:** Use `train_test_split` to divide the dataset into training and test sets, with a `test_size` of 0.2 and `random_state` of 10.\n",
        "2. **Set Up GridSearchCV:** Configure GridSearchCV with the Gradient Boosting Classifier, specifying the hyperparameter grid and setting `cv=2` for cross-validation.\n",
        "3. **Define the Hyperparameter Grid:** Use the given parameter grid to explore different configurations.\n",
        "4. **Train and Evaluate:** Execute the GridSearchCV to find the best hyperparameters based on cross-validation performance.\n",
        "5. **Report Optimal Hyperparameters:** Identify and report the hyperparameters that led to the best model performance.\n",
        "\n",
        "#### Hyperparameter Grid:\n",
        "```python\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [5, 8],\n",
        "    'min_samples_split': [2, 4, 6]\n",
        "}\n",
        "```\n",
        "\n",
        "#### Question:\n",
        "After conducting the grid search, what are the optimal hyperparameters (Rank 1 parameters) discovered for the Gradient Boosting Classifier?\n",
        "\n",
        "#### Options:\n",
        "A) {'max_depth': 5, 'min_samples_split': 6, 'n_estimators': 300}\n",
        "\n",
        "B)    {'max_depth': 8, 'min_samples_split': 6, 'n_estimators': 200}\n",
        "\n",
        "C)    {'max_depth': 5, 'min_samples_split': 4, 'n_estimators': 300}\n",
        "\n",
        "D)    {'max_depth': 8, 'min_samples_split': 6, 'n_estimators': 300}\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l5oeVyeCOhrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.______ import GridSearchCV\n",
        "\n",
        "\n",
        "# Defining the parameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [5, 8],\n",
        "    'min_samples_split': [2, 4, 6]\n",
        "}\n",
        "\n",
        "# Initializing the Gradient Boosting Classifier\n",
        "gbm = GradientBoostingClassifier(random_state=10)\n",
        "\n",
        "# Setting up the grid search with cross-validation\n",
        "grid_search = GridSearchCV(___, param_grid, _____ = \"accuracy\", ___=2, n_jobs = -1, verbose = 1)\n",
        "\n",
        "\n",
        "# Fitting the grid search to the data\n",
        "grid_search.___(____, ___)\n",
        "\n",
        "# Printing the best parameters and the corresponding score\n",
        "print(f\"Best parameters: {grid_search._____}\")\n",
        "print(f\"Best cross-validation score: {grid_search.best_score_}\")\n",
        "\n",
        "result = grid_search.cv_results_\n",
        "for i in range(len(result[\"params\"])):\n",
        "  print(f\"Parameters:{result['params'][i]} Mean_score: {result['mean_test_score'][i]} Rank: {result['rank_test_score'][i]}\")\n",
        "\n",
        "print(grid_search.best_estimator_)"
      ],
      "metadata": {
        "id": "K3FT8eRvXnIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Top Ranked Parameters\n",
        "\n",
        "#### Context:\n",
        "After optimizing the parameters for a Gradient Boosting Classifier using GridSearchCV, it's crucial to validate the effectiveness of these parameters beyond just cross-validation scores, specifically on a held-out test set.\n",
        "\n",
        "#### Task:\n",
        "Train multiple Gradient Boosting Classifier models using the top 5 ranked parameter sets from a previous GridSearchCV optimization. Evaluate each model's performance on a separate test set to determine which set achieves the highest accuracy.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Train Models with Top 5 Parameters:** Using the results from the previous GridSearchCV, identify the top 5 parameter sets based on cross-validation scores. Train a new model for each of these parameter sets on the training data.\n",
        "2. **Evaluate on Test Set:** Assess the performance of each model on the test set using accuracy as the metric.\n",
        "3. **Rank the Test Accuracies:** Determine the rank of each model based on test set accuracy, identifying which parameter set performed best.\n",
        "\n",
        "#### Question:\n",
        "After evaluating the test set accuracies for Gradient Boosting Classifiers trained with the top 5 parameter sets, what is the rank of the best performing model?\n",
        "\n",
        "#### Options:\n",
        "A) 1\n",
        "\n",
        "B) 2\n",
        "\n",
        "C) 3\n",
        "\n",
        "D) 5\n",
        "\n"
      ],
      "metadata": {
        "id": "GKvwwRLR9H5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Sort the scores and parameters by rank\n",
        "sorted_indices = np.____(result['rank_test_score'])\n",
        "top_indices = sorted_indices[___]  # Hint: Get top 5 indices\n",
        "\n",
        "# TODO: Train a model for each of the top 5 parameter sets\n",
        "for i in top_indices:\n",
        "    params = result['____'][i]\n",
        "    print(f\"Training model number {i} with parameters: {params}\")\n",
        "\n",
        "    # TODO: Initialize and train the Gradient Boosting Classifier with the current set of parameters\n",
        "    model = GradientBoostingClassifier(**____, random_state = 10)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # TODo: Evaluate the model on the test set\n",
        "    score = model.____(X_test, y_test)\n",
        "    print(f\"Test score with parameters {params}: {score}\\n\")"
      ],
      "metadata": {
        "id": "pP77tXEM5tIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Identifying Outliers\n",
        "\n",
        "#### Context:\n",
        "Outliers can significantly influence the performance of machine learning models, especially those not robust to extreme variations. Identifying outliers in training data helps in understanding their potential impact and deciding whether they should be treated or left as is.\n",
        "\n",
        "#### Task:\n",
        "Determine the number of outliers in the 'lead_time' feature of the training dataset, based on specified percentile thresholds.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Split the Data:** Use `train_test_split` to divide the dataset into training and test sets, setting `test_size` to 0.2 and `random_state` to 10.\n",
        "2. **Identify Outliers:** Calculate the 1st and 99th percentiles for the 'lead_time' feature in the training data and count how many values fall outside this range.\n",
        "3. **Count Outliers:** Determine the total number of outliers based on these thresholds.\n",
        "\n",
        "#### Question:\n",
        "Based on the 'lead_time' feature, how many outliers are present in the X_train set after splitting the dataset?\n",
        "\n",
        "#### Options:\n",
        "A) 0\n",
        "\n",
        "B) 344\n",
        "\n",
        "C) 172\n",
        "\n",
        "D) 224\n",
        "\n"
      ],
      "metadata": {
        "id": "YFHMDHlCAh_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Determine the 1th and 99th percentiles for 'lead_time'\n",
        "low_threshold, high_threshold = np._____(X_train['lead_time'], [___, __])  # Hint: Use the numpy function to calculate percentiles. Remember to specify the percentile values as a list.\n",
        "\n",
        "# Identify the indices of rows that are not considered outliers\n",
        "not_outliers = X_train['lead_time'].____(low_threshold, high_threshold)   # Hint: Use a comparison operation to generate a boolean mask where values are between the low and high thresholds.\n",
        "\n",
        "# Count the number of outliers\n",
        "num_outliers = np.__(~not_outliers)\n",
        "\n",
        "print(num_outliers)"
      ],
      "metadata": {
        "id": "y9kwZXEE7UTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Impact of Outlier Removal\n",
        "\n",
        "#### Context:\n",
        "Outlier removal can potentially improve the performance of machine learning models by reducing noise and anomalies that can mislead the training process. This task involves assessing the effect of outlier removal on the accuracy of a Gradient Boosting Classifier.\n",
        "\n",
        "#### Task:\n",
        "Train a Gradient Boosting Classifier on a dataset from which outliers have been removed and compare its performance with a model trained on the original dataset.\n",
        "\n",
        "#### Steps:\n",
        "1. **Prepare and Clean Data:** Remove outliers from `X_train` based on the 'lead_time' feature using previously identified thresholds. Assume outliers have been identified using the 1st and 99th percentiles.\n",
        "2. **Train Models:**\n",
        "   - Train a model on the dataset with outliers removed.\n",
        "   - Train another model on the original dataset for comparison.\n",
        "3. **Evaluate Models:** Assess both models' performance using accuracy on `X_test`.\n",
        "4. **Compare Performances:** Analyze whether removing outliers improves, reduces, or has no impact on model accuracy.\n",
        "\n",
        "#### Question:\n",
        "After removing outliers based on the 'lead_time' feature and training a new Gradient Boosting Classifier, what is the impact on the model's accuracy when evaluated on `X_test`?\n",
        "\n",
        "#### Options:\n",
        "A) Higher accuracy with outlier removal.\n",
        "\n",
        "B) Lower accuracy with outlier removal.\n",
        "\n",
        "C) No change in accuracy.\n",
        "\n",
        "D) Cannot determine without more details.\n",
        "\n"
      ],
      "metadata": {
        "id": "gs_UknG6e5Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Train on the original dataset\n",
        "original_clf = GradientBoostingClassifier(random_state=10)\n",
        "original_clf.fit(X_train, ____)\n",
        "original_predictions = original_clf.predict(X_test)\n",
        "original_accuracy = accuracy_score(_____, original_predictions)\n",
        "print(f\"Original dataset accuracy: {original_accuracy}\")\n",
        "\n",
        "# TODO: Filter the training data to remove outliers\n",
        "X_train_no_outliers = X_train[____]\n",
        "y_train_no_outliers = y_train[____]\n",
        "\n",
        "# Ensure there is data after removing outliers\n",
        "if len(X_train_no_outliers) == 0:\n",
        "    print(\"No data left after removing outliers based on 'lead_time'. Consider adjusting the percentile thresholds.\")\n",
        "else:\n",
        "    # TODO: Train on the dataset without outliers\n",
        "    clf_no_outliers = GradientBoostingClassifier(random_state=10)\n",
        "    clf_no_outliers.fit(_____, y_train_no_outliers)\n",
        "    no_outliers_predictions = ____.predict(X_test)\n",
        "    no_outliers_accuracy = accuracy_score(y_test, ______)\n",
        "    print(f\"Dataset without outliers (based on 'lead_time') accuracy: {no_outliers_accuracy}\")\n",
        "\n",
        "    # TODO: Compare the performance\n",
        "    if no_outliers_accuracy ___ original_accuracy:          # Hint: add the necessary condition\n",
        "        print(\"Removing outliers based on 'lead_time' improved the model's accuracy.\")\n",
        "    else:\n",
        "        print(\"Removing outliers based on 'lead_time' did not improve the model's accuracy.\")"
      ],
      "metadata": {
        "id": "wkA-8JK58CcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "### Comparing Loss Functions\n",
        "\n",
        "#### Context:\n",
        "The choice of loss function in a Gradient Boosting Classifier can influence its performance. Typically, \"log loss\" is used for classification, but \"exponential\" loss, which leads to a model similar to AdaBoost, can also be used.\n",
        "\n",
        "#### Task:\n",
        "Evaluate the performance impact of different loss functions in a Gradient Boosting Classifier. Train two versions of the classifier on the same dataset: one using the default \"log loss\" loss function and another using the \"exponential\" loss function, then compare their accuracies.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Prepare Data:** Split the dataset into training and test sets using `train_test_split`, setting `test_size` to 0.2 and `random_state` to 10.\n",
        "2. **Train with Default Loss Function:**\n",
        "   - Train a Gradient Boosting Classifier using the default \"log loss\" loss function.\n",
        "   - Evaluate its accuracy on `X_test`.\n",
        "3. **Train with Exponential Loss Function:**\n",
        "   - Train another Gradient Boosting Classifier, setting the `loss` parameter to \"exponential\".\n",
        "   - Evaluate its accuracy on `X_test`.\n",
        "4. **Compare Performances:** Assess whether the model using \"exponential\" loss achieves higher, lower, or the same accuracy as the model using \"log loss\" loss.\n",
        "\n",
        "#### Question:\n",
        "How does the accuracy of the Gradient Boosting Classifier using the exponential loss function compare to the model using the default log_loss function when evaluated on `X_test`?\n",
        "\n",
        "#### Options:\n",
        "A) Cannot determine without more details.\n",
        "\n",
        "B) The log loss loss model is more accurate.\n",
        "\n",
        "C) Both models have the same accuracy.\n",
        "\n",
        "D) The exponential loss model is more accurate.\n",
        "\n"
      ],
      "metadata": {
        "id": "XH2jC8_MAoci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Train a Gradient Boosting Classifier using the default loss function (which is 'log_loss')\n",
        "clf_log_loss = GradientBoostingClassifier(____='log_loss', random_state=10)\n",
        "clf_log_loss.___(X_train, ____)\n",
        "predictions_log_loss = clf_log_loss.______(X_test)\n",
        "accuracy_log_loss = accuracy_score(y_test, predictions_log_loss)\n",
        "print(f\"Accuracy with the 'log_loss' loss function: {accuracy_log_loss}\")\n",
        "\n",
        "# TODO: Train a Gradient Boosting Classifier using the 'exponential' loss function\n",
        "clf_exponential = GradientBoostingClassifier(loss='_____', random_state=10)\n",
        "_____.fit(X_train, y_train)\n",
        "predictions_exponential = _____.predict(X_test)\n",
        "accuracy_exponential = ______(____, predictions_exponential)\n",
        "print(f\"Accuracy with the 'exponential' loss function: {accuracy_exponential}\")\n",
        "\n",
        "# Compare the accuracies\n",
        "if accuracy_exponential > accuracy_log_loss:\n",
        "    print(\"The 'exponential' loss function model is more accurate on the test set.\")\n",
        "elif accuracy_exponential < accuracy_log_loss:\n",
        "    print(\"The 'log_loss' loss function model is more accurate on the test set.\")\n",
        "else:\n",
        "    print(\"Both models have the same accuracy on the test set.\")"
      ],
      "metadata": {
        "id": "keg1HIFhB1WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Evaluating the Impact of Increasing Weak Learners\n",
        "\n",
        "#### Context:\n",
        "In Gradient Boosting, the number of weak learners (trees) is a crucial hyperparameter. It controls the complexity of the model and can significantly impact its performance. Observing how the model's accuracy evolves with the addition of more trees can provide insights into its learning dynamics and potential overfitting.\n",
        "\n",
        "#### Task:\n",
        "Train a series of Gradient Boosting Classifiers with an increasing number of estimators to evaluate how the addition of trees affects the test accuracy on a given dataset.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Prepare Data:** Split the dataset into training and test sets using `train_test_split`, setting `test_size` to 0.2 and `random_state` to 10.\n",
        "2. **Initialize and Train Models:**\n",
        "   - Start with a Gradient Boosting Classifier with `n_estimators=1`.\n",
        "   - Incrementally increase `n_estimators` through the set [1, 5, 10, 20, 50, 100, 200, 300, 500, 1000], training a new model at each step.\n",
        "3. **Evaluate Each Model:** Assess the accuracy of each model on `X_test`.\n",
        "4. **Record and Analyze Results:** Track how the test accuracy changes as more estimators are added. Look for trends such as increases, plateaus, or potential decreases in accuracy which could suggest overfitting.\n",
        "\n",
        "\n",
        "#### Question:\n",
        "How does the addition of more weak learners (trees) affect the test accuracy of the Gradient Boosting Classifier?\n",
        "\n",
        "#### Options:\n",
        "A) Test accuracy decreases as more weak learners are added.\n",
        "\n",
        "B) Test accuracy initially increases but then plateaus or decreases slightly, indicating potential overfitting.\n",
        "\n",
        "C) Test accuracy consistently increases with more weak learners.\n",
        "\n",
        "D) There is no clear pattern; the test accuracy changes unpredictably.\n",
        "\n"
      ],
      "metadata": {
        "id": "VV0e1q3WD1Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the number of estimators\n",
        "n_estimators_list = [1, 5, 10, 20, 50, 100, 200, 300, 500, 1000]\n",
        "test_accuracies = []\n",
        "\n",
        "# TODO: Train and evaluate the model with different numbers of weak learners\n",
        "for n_estimators in _____:\n",
        "    clf = GradientBoostingClassifier(_____=n_estimators, random_state=10)\n",
        "    clf.fit(_____, y_train)\n",
        "    predictions = clf.____(X_test)\n",
        "    accuracy = accuracy_score(_____, predictions)\n",
        "    test_accuracies.append(accuracy)\n",
        "    print(f\"Accuracy with {n_estimators} estimators: {accuracy}\")\n",
        "\n",
        "# Plotting the test accuracies\n",
        "plt.plot(n_estimators_list, test_accuracies, marker='o')\n",
        "plt.xlabel('Number of Estimators')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('Test Accuracy vs. Number of Estimators in Gradient Boosting')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CQAirHZ1A7FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Other Boosting Techniques"
      ],
      "metadata": {
        "id": "QAG2YvNaL6X-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Comparison Between XGBoost and GBC\n",
        "\n",
        "\n",
        "#### Context:\n",
        "XGBoost and Gradient Boosting Classifier (GBC) are both powerful ensemble machine learning algorithms based on boosting. While both aim to sequentially correct errors of weak learners, they are implemented differently and may have variations in performance, training speed, and memory usage.\n",
        "\n",
        "#### Task:\n",
        "Train both an XGBoost model and a Gradient Boosting Classifier on a dataset, then compare their training time and the file sizes of the saved models to determine which model is more efficient in terms of speed and storage.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Split the Data:** Use `train_test_split` to divide the dataset into training and test sets, with a `test_size` of 0.2 and `random_state` of 10.\n",
        "2. **Train Models:**\n",
        "   - Train an XGBoost model using default parameters.\n",
        "   - Train a Gradient Boosting Classifier using default parameters.\n",
        "   - Record the training time for each model.\n",
        "3. **Serialize Models:**\n",
        "   - Save both models to disk using `pickle`.\n",
        "   - Record the file size of each saved model.\n",
        "4. **Evaluate and Compare:**\n",
        "   - Compare the training times and file sizes to determine which model is more efficient.\n",
        "\n",
        "#### Question:\n",
        "After performing the training and saving the models, which of the following statements is correct regarding the performance and efficiency of XGBoost and GBC?\n",
        "\n",
        "#### Options:\n",
        "A) XGBoost trains faster and results in a smaller file size.\n",
        "\n",
        "B) GBC trains faster and results in a smaller file size.\n",
        "\n",
        "C) XGBoost trains faster but GBC results in a smaller file size.\n",
        "\n",
        "D) GBC trains faster but XGBoost results in a smaller file size.\n",
        "\n"
      ],
      "metadata": {
        "id": "8qzTQPgqGNZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pickle\n",
        "from _____ import XGBClassifier\n",
        "import os\n",
        "\n",
        "# TODO: Train XGBoost\n",
        "start_time = time.time()\n",
        "xgb_model = _____(random_state=10)\n",
        "xgb_model.____(X_train, y_train)\n",
        "xgb_train_time = time.time() - start_time\n",
        "\n",
        "# Save XGBoost model\n",
        "with open('xgb_model.pkl', 'wb') as file:\n",
        "    pickle.dump(xgb_model, file)\n",
        "\n",
        "# TODO: Train GBC\n",
        "start_time = ____.time()\n",
        "gbc_model = _____(random_state=10)\n",
        "gbc_model.fit(X_train, ____)\n",
        "gbc_train_time = time.time() - _____\n",
        "\n",
        "# Save GBC model\n",
        "with open('gbc_model.pkl', 'wb') as file:\n",
        "    pickle.dump(gbc_model, file)\n",
        "\n",
        "# Output the training times\n",
        "print(f\"XGBoost training time: {xgb_train_time} seconds\")\n",
        "print(f\"GBC training time: {gbc_train_time} seconds\")\n",
        "\n",
        "# Check the file sizes\n",
        "xgb_file_size = os.path.getsize('xgb_model.pkl')\n",
        "gbc_file_size = os.path.getsize('gbc_model.pkl')\n",
        "\n",
        "# Output the file sizes\n",
        "print(f\"XGBoost model file size: {xgb_file_size} bytes\")\n",
        "print(f\"GBC model file size: {gbc_file_size} bytes\")"
      ],
      "metadata": {
        "id": "47zf0WbND7lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Comparing Feature Importance\n",
        "\n",
        "#### Context:\n",
        "Understanding how different boosting algorithms prioritize features and their prediction accuracy is crucial when choosing a model for deployment. This task involves evaluating XGBoost and Gradient Boosting Classifier (GBC) to determine which model performs better in terms of accuracy and how they compare in terms of identifying the most important features.\n",
        "\n",
        "#### Task:\n",
        "Train both XGBoost and a Gradient Boosting Classifier on the Reservation Booking Status dataset, evaluate their accuracy on the test set, and compare the most important features identified by each model.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Prepare Data:** Split the dataset into training and test sets using `train_test_split`, with a `test_size` of 0.2 and `random_state` of 10.\n",
        "2. **Train Models:**\n",
        "    - Train an XGBoost model using default parameters.\n",
        "    - Train a Gradient Boosting Classifier using default parameters.\n",
        "    - Record the accuracy of each model on the test set.\n",
        "3. **Evaluate Feature Importance:**\n",
        "    - Determine the most important feature for each model.\n",
        "4. **Compare Models:**\n",
        "    - Compare the accuracies and most important features of the two models.\n",
        "\n",
        "#### Question:\n",
        "After evaluating both models on the test set and assessing the most important features, which of the following statements is correct?\n",
        "\n",
        "#### Options:\n",
        "A) XGBoost has higher accuracy and both models agree on the most important feature.\n",
        "\n",
        "B) GBC has higher accuracy and both models agree on the most important feature.\n",
        "\n",
        "C) XGBoost has higher accuracy, but the models disagree on the most important feature.\n",
        "\n",
        "D) GBC has higher accuracy, but the models disagree on the most important feature.\n"
      ],
      "metadata": {
        "id": "40M3Kx9ZLaXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODD: Evaluate XGBoost on the test set\n",
        "xgb_predictions = xgb_model.____(X_test)\n",
        "xgb_accuracy = accuracy_score(_____, xgb_predictions)\n",
        "print(f\"XGBoost accuracy: {xgb_accuracy}\")\n",
        "\n",
        "# TODD: Determine the most important feature in XGBoost\n",
        "xgb_feature_importances = xgb_model._____\n",
        "xgb_most_important_feature = ______\n",
        "print(f\"Most important feature in XGBoost: {xgb_most_important_feature}\")\n",
        "\n",
        "# TODD: Evaluate GBC on the test set\n",
        "gbc_predictions = gbc_model.____(X_test)\n",
        "gbc_accuracy = accuracy_score(____, gbc_predictions)\n",
        "print(f\"GBC accuracy: {gbc_accuracy}\")\n",
        "\n",
        "# TODD: Determine the most important feature in GBC\n",
        "gbc_feature_importances = gbc_model.______\n",
        "gbc_most_important_feature =  _____\n",
        "print(f\"Most important feature in GBC: {gbc_most_important_feature}\")\n",
        "\n",
        "# Feature importance for XGBoost\n",
        "indices_xgb = np.argsort(xgb_feature_importances)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Feature importances in XGBoost\")\n",
        "plt.barh(range(X_train.shape[1]), xgb_feature_importances[indices_xgb], align=\"center\")\n",
        "plt.yticks(range(X_train.shape[1]), X.columns[indices_xgb])\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Feature importance for GBC\n",
        "indices_gbc = np.argsort(gbc_feature_importances)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Feature importances in GBC\")\n",
        "plt.barh(range(X_train.shape[1]), gbc_feature_importances[indices_gbc], align=\"center\")\n",
        "plt.yticks(range(X_train.shape[1]), X.columns[indices_gbc])\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Features')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RwZD-ovQIdLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "### Matching Characteristics to XGBoost and LightGBM\n",
        "\n",
        "#### Context:\n",
        "Understanding the fundamental differences between boosting algorithms like XGBoost and LightGBM is crucial for making informed decisions about which to use based on the dataset characteristics. This task involves matching specific features of these models to their respective algorithms.\n",
        "\n",
        "#### Task:\n",
        "Correctly identify which characteristics apply to XGBoost and which apply to LightGBM.\n",
        "\n",
        "\n",
        "\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "<meta charset=\"UTF-8\">\n",
        "<title>Match the Following Characteristics</title>\n",
        "<style>\n",
        "  .matching-table {\n",
        "    width: 100%;\n",
        "    border-collapse: collapse;\n",
        "  }\n",
        "  \n",
        "  .matching-table th,\n",
        "  .matching-table td {\n",
        "    border: 1px solid black;\n",
        "    padding: 5px;\n",
        "    text-align: left;\n",
        "  }\n",
        "  \n",
        "  .matching-table th {\n",
        "    background-color: #f2f2f2;\n",
        "  }\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "\n",
        "<table class=\"matching-table\">\n",
        "  <tr>\n",
        "    <th>Models</th>\n",
        "    <th>Characteristics</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>A. XGBoost</td>\n",
        "    <td>I. Uses level-wise tree growth.</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td></td>\n",
        "    <td>II. Uses leaf-wise tree growth.</td>\n",
        "  </tr>  \n",
        "\t<tr>\n",
        "    <td>B. LightGBM</td>\n",
        "    <td>III. Requires one-hot encoding for categorical features.</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td></td>\n",
        "    <td>IV: Can handle categorical features natively without one-hot encoding.</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "</body>\n",
        "</html>\n",
        "\n",
        "#### Question:\n",
        "Match the following characteristics to XGBoost and LightGBM:\n",
        "\n",
        "#### Options:\n",
        "A) `[A — I, II] [B — III, IV]`\n",
        "\n",
        "B) `[A — II, IV] [B — I, III]`\n",
        "\n",
        "C) `[A — I, III] [B — II, IV]`\n",
        "\n",
        "D) `[A — I, IV] [B — II, III]`"
      ],
      "metadata": {
        "id": "oizho8yIEL4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Comparing XGBoost and LightGBM\n",
        "\n",
        "#### Context:\n",
        "XGBoost and LightGBM are both popular gradient boosting frameworks that are widely used due to their performance and speed. Comparing these two models on the same dataset can provide valuable insights into their efficiency and effectiveness in predictive accuracy.\n",
        "\n",
        "#### Task:\n",
        "Train both XGBoost and LightGBM models using default parameters on the Reservation Booking Status dataset. Record and compare their training times and accuracies.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Prepare Data:** Split the dataset into training and test sets using `train_test_split`, setting `test_size` to 0.2 and `random_state` to 10.\n",
        "2. **Train Models:**\n",
        "   - Train an XGBoost model and record the training time.\n",
        "   - Train a LightGBM model and record the training time.\n",
        "3. **Evaluate Models:** Measure the accuracy of each model on the test set.\n",
        "4. **Report Findings:** Compare the training times and accuracies of XGBoost and LightGBM.\n",
        "\n",
        "#### Question:\n",
        "After training both models and evaluating them on the test set, which of the following statements is true?\n",
        "\n",
        "#### Options:\n",
        "A) XGBoost trains faster and has higher accuracy.\n",
        "\n",
        "B) LightGBM trains faster and has higher accuracy.\n",
        "\n",
        "C) XGBoost trains faster, but LightGBM has higher accuracy.\n",
        "\n",
        "D) LightGBM trains faster, but XGBoost has higher accuracy.\n"
      ],
      "metadata": {
        "id": "AV1-A9fNWVlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ____ import LGBMClassifier\n",
        "\n",
        "# Train XGBoost\n",
        "start_time = ___.___()\n",
        "xgb_model = XGBClassifier(random_state=10)\n",
        "xgb_model.fit(___, ____)\n",
        "xgb_train_time = time.time() - start_time\n",
        "xgb_predictions = xgb_model.____(X_test)\n",
        "xgb_accuracy = accuracy_score(____, xgb_predictions)\n",
        "\n",
        "# Train LightGBM\n",
        "start_time = ___.___()\n",
        "lgb_model = LGBMClassifier(random_state=10)\n",
        "lgb_model.fit(___, ___)\n",
        "lgb_train_time = time.time() - ____\n",
        "lgb_predictions = lgb_model.___(X_test)\n",
        "lgb_accuracy = accuracy_score(___, lgb_predictions)\n",
        "\n",
        "# Output the results\n",
        "print(f\"XGBoost training time: {xgb_train_time} seconds\")\n",
        "print(f\"XGBoost test accuracy: {xgb_accuracy}\")\n",
        "print(f\"LightGBM training time: {lgb_train_time} seconds\")\n",
        "print(f\"LightGBM test accuracy: {lgb_accuracy}\")"
      ],
      "metadata": {
        "id": "8IbBTI9uSzF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Comparing Stacking and Voting Ensemble Models\n",
        "\n",
        "#### Context:\n",
        "Stacking and voting are two ensemble techniques used to improve machine learning predictions by combining the strengths of multiple models. This task will explore their efficacy by using them in a classification problem with several popular algorithms as base learners.\n",
        "\n",
        "#### Task:\n",
        "Implement a stacking ensemble model and a voting ensemble model using the same base learners on the Reservation Booking Status dataset. Compare the accuracy of these two ensemble methods on the test set.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Load and Preprocess Data:**\n",
        "    - Ensure that the Reservation Booking Status dataset is clean and prepared for modeling.\n",
        "   \n",
        "2. **Split the Data:**\n",
        "    - Use `train_test_split` to divide the data into training and test sets, ensuring `random_state=10` for reproducibility.\n",
        "\n",
        "3. **Train Base Learners:**\n",
        "    - Initialize and train Random Forest, Gradient Boosting Classifier, XGBoost, LightGBM, and AdaBoost. Set `random_state=10` where applicable to ensure consistent results.\n",
        "\n",
        "4. **Implement Stacking Ensemble:**\n",
        "    - Combine the predictions of the base learners to create a new training dataset for the meta-learner, XGBoost.\n",
        "    - Train the XGBoost meta-learner on this new dataset.\n",
        "    - Evaluate the stacking ensemble model on the test set.\n",
        "\n",
        "5. **Implement Voting Ensemble:**\n",
        "    - Combine the base learners using a voting mechanism.\n",
        "    - Evaluate the voting ensemble model on the test set.\n",
        "\n",
        "6. **Compare Accuracies:**\n",
        "    - Determine which ensemble model performs better on the test set.\n",
        "\n",
        "\n",
        "\n",
        "#### Question:\n",
        "After implementing and evaluating both the stacking ensemble model with XGBoost as the meta-learner and the voting ensemble model, how do their accuracies compare on the test set?\n",
        "\n",
        "#### Options:\n",
        "A) The stacking ensemble model is more accurate than the voting ensemble model.\n",
        "\n",
        "B) The voting ensemble model is more accurate than the stacking ensemble model.\n",
        "\n",
        "C) Both models achieve the same accuracy on the test set.\n",
        "\n",
        "D) The accuracy comparison cannot be determined without further information.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oAAvN6FtMhE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.___ import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Initialize the base learners\n",
        "rf = RandomForestClassifier(random_state=10)\n",
        "gbc = ___(random_state=10)\n",
        "xgb = ____(random_state=10)\n",
        "lgbm = ___(random_state=10)\n",
        "ada = AdaBoostClassifier(random_state=10)\n",
        "\n",
        "# Initialize the Stacking Classifier using XGBoost as meta-learner\n",
        "stacking_clf = StackingClassifier(_____=[rf, gbc, lgbm, ada],\n",
        "                                  meta_classifier=____,\n",
        "                                  use_probas=True,\n",
        "                                  average_probas=False)\n",
        "\n",
        "# Train the stacking classifier\n",
        "stacking_clf.fit(X_train, ____)\n",
        "\n",
        "# Evaluate the stacking model\n",
        "stacking_predictions = stacking_clf.____(X_test)\n",
        "stacking_accuracy = accuracy_score(____, stacking_predictions)\n",
        "print(\"---\"*10)\n",
        "print(f\"Stacking Model Accuracy: {stacking_accuracy}\")\n",
        "print(\"---\"*10)\n",
        "\n",
        "# Initialize the Voting Classifier\n",
        "voting_clf = VotingClassifier(_____=[\n",
        "    ('rf', rf), ('gbc', gbc), ('xgb', xgb), ('lgbm', lgbm), ('ada', ada)\n",
        "])\n",
        "\n",
        "# Train the voting classifier\n",
        "voting_clf.fit(____, y_train)\n",
        "\n",
        "# Evaluate the voting model\n",
        "voting_predictions = voting_clf.predict(___)\n",
        "voting_accuracy = accuracy_score(y_test, _____)\n",
        "print(\"---\"*10)\n",
        "print(f\"Voting Model Accuracy: {voting_accuracy}\")\n",
        "\n",
        "# Compare the accuracies\n",
        "print(\"---\"*10)\n",
        "if stacking_accuracy > voting_accuracy:\n",
        "    print(\"Stacking ensemble is more accurate.\")\n",
        "elif stacking_accuracy < voting_accuracy:\n",
        "    print(\"Voting ensemble is more accurate.\")\n",
        "else:\n",
        "    print(\"Both ensembles have the same accuracy.\")\n",
        "print(\"---\"*10)"
      ],
      "metadata": {
        "id": "Xx18BOZMLuic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Implementing a Cascading Boosting Model\n",
        "\n",
        "#### Context:\n",
        "Cascading models involve a sequential application of models where the output of one model feeds into the next. This technique can help focus a more complex model on the harder cases that a simpler model struggles with, potentially improving overall prediction accuracy.\n",
        "\n",
        "#### Task:\n",
        "Create a two-stage cascading boosting model where the first stage uses a LightGBM classifier to preprocess the data, and the second stage trains an XGBoost classifier on instances for which the first model had lower confidence. Evaluate the final model's performance on the entire test set.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Data Preparation:**\n",
        "    - Split the dataset into training and test sets using `train_test_split` with a `test_size` of 0.2 and `random_state` of 10.\n",
        "\n",
        "2. **First Stage - LightGBM Classifier:**\n",
        "    - Train a LightGBM classifier on the training set.\n",
        "    - Obtain prediction probabilities for the training set and create a filter based on these probabilities (retain instances where the confidence is between 25% and 75%).\n",
        "\n",
        "3. **Second Stage - XGBoost Classifier:**\n",
        "    - Train an XGBoost classifier on the filtered training data.\n",
        "    - Evaluate the classifier's performance on the entire test set using accuracy as the metric.\n",
        "\n",
        "#### Question:\n",
        "After implementing the cascading model where the first stage is a LightGBM classifier and the second stage is an XGBoost classifier trained on filtered instances, what is the accuracy of the final XGBoost model on the entire test set?\n",
        "\n",
        "#### Options:\n",
        "A) Above 90%\n",
        "\n",
        "B) Between 80% and 90%\n",
        "\n",
        "C) Between 70% and 80%\n",
        "\n",
        "D) Below 70%\n",
        "\n"
      ],
      "metadata": {
        "id": "qGrvIiTMeWay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Layer 1: Train a LightGBM classifier\n",
        "lgb_clf = LGBMClassifier(random_state=10)\n",
        "lgb_clf.fit(___, ___)\n",
        "\n",
        "# TODO: Get the prediction probabilities for the training set and testing set\n",
        "lgb_train_prob = lgb_clf.____(X_train)[:, 1]   # Hint: we want probabilities not the predictions\n",
        "lgb_test_prob = lgb_clf.____(X_test)[:, 1]\n",
        "\n",
        "# Filter out instances with confidence above 75% and below 25%\n",
        "filter_threshold = 0.75\n",
        "\n",
        "# TODO: Filter the DataFrame based on the LightGBM model's prediction probabilities above 75% and below 25%\n",
        "filtered_train_df = X_train[(lgb_train_prob < _______) | (______ > (1 - filter_threshold))]\n",
        "filtered_y_train = y_train[(_______ < filter_threshold) | (lgb_train_prob > (1 - ______))]\n",
        "\n",
        "# TODO: Layer 2: Train a XGBoost classifier on the filtered training set\n",
        "xgb_clf = XGBClassifier(random_state=10)\n",
        "xgb_clf.fit(______, ______)\n",
        "\n",
        "# TODO: Make final predictions on the  test set with the XGBoost model\n",
        "final_predictions = xgb_clf.predict(_____)\n",
        "final_accuracy = accuracy_score(y_test, final_predictions)\n",
        "print(f\"Final Model Accuracy on Filtered Test Set: {final_accuracy}\")"
      ],
      "metadata": {
        "id": "w9chJ_E_ZbZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Matching Boosting Techniques\n",
        "\n",
        "#### Context:\n",
        "Boosting is a powerful ensemble technique that combines multiple weak learners to create a strong learner. Various boosting algorithms have unique characteristics that make them suitable for specific types of data or problems. This question aims to match each boosting technique with its defining feature.\n",
        "\n",
        "#### Task:\n",
        "Identify the unique specialty of each listed boosting technique based on its implementation details and typical use cases.\n",
        "\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "<meta charset=\"UTF-8\">\n",
        "<style>\n",
        "  .matching-table {\n",
        "    width: 100%;\n",
        "    border-collapse: collapse;\n",
        "  }\n",
        "  \n",
        "  .matching-table th,\n",
        "  .matching-table td {\n",
        "    border: 1px solid black;\n",
        "    padding: 5px;\n",
        "    text-align: left;\n",
        "  }\n",
        "  \n",
        "  .matching-table th {\n",
        "    background-color: #f2f2f2;\n",
        "  }\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "\n",
        "\n",
        "<table class=\"matching-table\">\n",
        "  <tr>\n",
        "    <th>Boosting Techniques</th>\n",
        "    <th>Unique Specialties</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1. Gradient Boosting</td>\n",
        "    <td>A. Utilizes gradient-based optimization and works with a variety of differentiable loss functions.</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2. AdaBoost</td>\n",
        "    <td>B. Employs symmetric trees with a special scoring function that counteracts the bias in decision trees with respect to categorical features.</td>\n",
        "  </tr>  \n",
        "  <tr>\n",
        "    <td>3. XGBoost</td>\n",
        "    <td>C. Introduces a regularization term in the objective function to control over-fitting, making it robust.</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4. LightGBM</td>\n",
        "    <td>D. Utilizes a histogram-based algorithm that reduces memory usage and speeds up computation.</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>5. CatBoost</td>\n",
        "    <td>E. Uses an exponential loss function and focuses on reweighting misclassified instances after each iteration.</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "</body>\n",
        "</html>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Question:\n",
        "Match each boosting technique to the correct specialty that distinguishes it from the others, based on the description given.\n",
        "\n",
        "#### Options:\n",
        "A) `[1 - A, 2 - E, 3 - C, 4 - D, 5 - B]`\n",
        "\n",
        "B) `[1 - C, 2 - A, 3 - E, 4 - B, 5 - D]`\n",
        "\n",
        "C) `[1 - D, 2 - B, 3 - A, 4 - E, 5 - C]`\n",
        "\n",
        "D) `[1 - E, 2 - D, 3 - B, 4 - C, 5 - A]`\n",
        "\n"
      ],
      "metadata": {
        "id": "wMn-NodTykCB"
      }
    }
  ]
}