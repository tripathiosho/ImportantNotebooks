{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cTW5ZNrc0hsy",
        "mWqrTpO91_LW",
        "mNqoWbeoXCjz"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tripathiosho/ImportantNotebooks/blob/main/Naive_Bayes_Case_Study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting Health Outcomes of Horses with Machine Learning\n",
        "\n",
        "### Context\n",
        "\n",
        "Welcome to the **Scaler Health Analytics** team! Our objective is to leverage advanced analytics to predict health outcomes in horses, enabling veterinarians to make more informed decisions that can improve treatment strategies and overall horse health management. Utilizing machine learning techniques, particularly classification algorithms, you will analyze clinical data from horses to predict various health outcomes.\n",
        "\n",
        "### Dataset Description\n",
        "\n",
        "You have been provided with a comprehensive dataset containing clinical records of horses treated in a veterinary hospital. The data encapsulates a variety of clinical attributes that offer insights into the general health and medical condition of each horse. Each record in the dataset is described by the following features:\n",
        "\n",
        "### Clinical Attributes:\n",
        "- **id:** Unique identifier for each horse.\n",
        "- **surgery:** Whether the horse had surgery (Yes/No).\n",
        "- **age:** Age of the horse.\n",
        "- **hospital_number:** Unique number assigned to the horse at the hospital.\n",
        "- **rectal_temp:** Rectal temperature of the horse.\n",
        "- **pulse:** Pulse rate in beats per minute.\n",
        "- **respiratory_rate:** Respiratory rate in breaths per minute.\n",
        "- **temp_of_extremities:** Temperature of extremities (a possible indicator of shock).\n",
        "- **peripheral_pulse:** Quality of the peripheral pulse.\n",
        "- **mucous_membrane:** Color of the mucous membranes, which can indicate blood circulation quality.\n",
        "- **capillary_refill_time:** Time taken for color to return to mucous membrane after pressure is applied.\n",
        "- **pain:** Horse's pain level (graded).\n",
        "- **peristalsis:** Intestinal activity observed.\n",
        "- **abdominal_distention:** Any distention of the abdomen.\n",
        "- **nasogastric_tube:** Whether a nasogastric tube has been placed.\n",
        "- **nasogastric_reflux:** Any nasogastric reflux noted.\n",
        "- **nasogastric_reflux_ph:** pH of the nasogastric reflux.\n",
        "- **rectal_exam_feces:** Findings of the rectal examination of feces.\n",
        "- **abdomen:** Detailed examination findings of the abdomen.\n",
        "- **packed_cell_volume:** Packed cell volume, indicating hydration status and blood loss.\n",
        "- **total_protein:** Total protein levels in blood.\n",
        "- **abdomo_appearance:** Appearance of abdominal fluid.\n",
        "- **abdomo_protein:** Protein level in abdominal fluid.\n",
        "- **surgical_lesion:** Presence of surgical lesions.\n",
        "- **lesion_1, lesion_2, lesion_3:** Type and location of lesions identified.\n",
        "- **cp_data:** Clinical pathology data.\n",
        "- **outcome:** Health outcome of the horse (e.g., recovered, euthanized, died).\n",
        "\n",
        "Your task is to use machine learning models to predict the 'outcome' for each horse and identify key predictors of health outcomes. This project will contribute significantly to improving the predictive models used in veterinary practices."
      ],
      "metadata": {
        "id": "cTW5ZNrc0hsy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYm6et5Bxufg"
      },
      "outputs": [],
      "source": [
        "!wget https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/072/101/original/health_outcome_dataset.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "p_QQa2BP24W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"health_outcome_dataset.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "0oXxhVW8zoEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "OnIhQ0UA9g1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "df.drop('lesion_3', inplace = True, axis = 1)\n",
        "\n",
        "# Create imputer objects\n",
        "num_imputer = SimpleImputer(strategy='median')  # Imputer for numerical data\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')  # Imputer for categorical data\n",
        "\n",
        "# Define columns by type\n",
        "num_cols = ['rectal_temp', 'pulse', 'respiratory_rate']  # Numerical columns\n",
        "cat_cols = ['temp_of_extremities', 'peripheral_pulse', 'mucous_membrane', 'capillary_refill_time',\n",
        "            'pain', 'peristalsis', 'abdominal_distention', 'nasogastric_tube',\n",
        "            'nasogastric_reflux', 'abdomen', 'abdomo_appearance', 'rectal_exam_feces']  # Categorical columns\n",
        "\n",
        "# Apply imputation\n",
        "df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
        "df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])"
      ],
      "metadata": {
        "id": "xxmaSkFe-9fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['outcome'].value_counts()"
      ],
      "metadata": {
        "id": "kIChJZ7TUypt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Assesments for Naive Bayes-I\n"
      ],
      "metadata": {
        "id": "mWqrTpO91_LW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### Pulse Rate Distribution\n",
        "\n",
        "#### Context:\n",
        "Analyzing the pulse rate distribution among horses with different health outcomes can provide critical insights into physiological states associated with various health conditions. This analysis can help veterinarians understand how pulse rate might correlate with the severity or type of condition a horse is facing.\n",
        "\n",
        "#### Task:\n",
        "Create a visual representation to analyze the pulse rate distribution across different health outcomes using the 'pulse' and 'outcome' columns in the dataset.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Group and Count Data:** Use the dataset to group entries by 'outcome' and calculate the average 'pulse' for each outcome category.\n",
        "2. **Visualize Data:** Generate a box plot to display the distribution of pulse rates for each health outcome category. This will help in visually comparing the pulse rate variability associated with different outcomes.\n",
        "3. **Analyze Trends:** Examine the box plot to determine which health outcome is associated with the highest average pulse rate and which shows the most variability in pulse rates.\n",
        "\n",
        "#### Question:\n",
        "After analyzing the box plot representing the pulse rate distribution for different health outcomes, identify the correct statements regarding pulse rates.\n",
        "\n",
        "#### Options:\n",
        "A) The average pulse rate is highest among horses that lived, indicating recovery from potentially distressing conditions.\n",
        "\n",
        "B) The average pulse rate is lowest for horses that were euthanized, suggesting less physiological distress before euthanasia.\n",
        "\n",
        "C) The average pulse rate is highest among horses that died, suggesting a correlation between high pulse rates and critical health conditions leading to death.\n",
        "\n",
        "D) The box plot shows no significant difference in the average pulse rates across different health outcomes."
      ],
      "metadata": {
        "id": "L7g1lN5b1iv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# TODO: Group the dataframe by 'outcome' and calculate the average 'pulse' for each outcome category.\n",
        "pulse_outcome_data = df.______  # Consider what function might summarize the data effectively.\n",
        "print(pulse_outcome_data)\n",
        "\n",
        "# Set a custom color palette (optional)\n",
        "custom_palette = sns.color_palette(\"pastel\")\n",
        "\n",
        "# TODO: Utilize seaborn to create a box plot that shows variation within grouped data.\n",
        "sns.boxplot(x=_____, y=______, data=df, palette=custom_palette)\n",
        "plt.title('Pulse Rate Distribution by Health Outcome')\n",
        "plt.xlabel('Health Outcome')\n",
        "plt.ylabel('Pulse Rate')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()  # Adjust layout to not cut off labels\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OwG1FsdlzvPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Rectal Temperature Correlation\n",
        "\n",
        "#### Context:\n",
        "Understanding how rectal temperature correlates with health outcomes in horses can provide valuable insights for veterinary treatment strategies. Rectal temperature is a critical clinical parameter and its deviations from the norm can indicate various health conditions.\n",
        "\n",
        "#### Task:\n",
        "Analyze the impact of different rectal temperature categories on health outcomes. The rectal temperatures will be categorized into hypothermia, normal, and fever to observe how these conditions correlate with health outcomes.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Create Temperature Categories:** Add a new column to the dataset, 'temp_category', which categorizes rectal temperature into hypothermia (<37.5°C), normal (37.5-38.5°C), and fever (>38.5°C).\n",
        "2. **Group Data by Temperature Category:** Group the dataset by 'temp_category' and 'outcome' to analyze the distribution of health outcomes within each temperature category.\n",
        "3. **Visualize and Analyze Data:** Create a bar chart to visualize the count of each health outcome within the different temperature categories. This will help identify any significant correlations between rectal temperature and health outcomes.\n",
        "\n",
        "#### Question:\n",
        "After analyzing the bar chart which depicts the count of health outcomes within each rectal temperature category, identify the correct statement regarding the correlation between rectal temperature categories and health outcomes.\n",
        "\n",
        "#### Options:\n",
        "A) Horses with fever have the highest survival rate.\n",
        "\n",
        "B) Normal rectal temperature is associated with the best health outcomes.\n",
        "\n",
        "C) Horses with hypothermia are more likely to be euthanized.\n",
        "\n",
        "D) There is no significant relationship between rectal temperature categories and health outcomes.\n"
      ],
      "metadata": {
        "id": "0YJZ51QR3Dxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to categorize rectal temperature\n",
        "def categorize_temp(temp):\n",
        "    # TODO: Return the category based on temperature values (use if-elif-else structure).\n",
        "    _____\n",
        "\n",
        "# Apply function to create a new column\n",
        "df['temp_category'] = df['rectal_temp'].____(categorize_temp)  # TODO: Choose the appropriate DataFrame method to apply a function.\n",
        "\n",
        "# Grouping data by temperature category and outcome for plotting\n",
        "grouped_data = df.groupby(______)._____()['id'].reset_index()  # TODO: Group by two columns and count a unique identifier.\n",
        "grouped_data.columns = ['Temperature Category', 'Outcome', 'Count']\n",
        "\n",
        "# Create a pivot table for visualization\n",
        "pivot_table = pd.pivot_table(grouped_data, values=____, index=_____, columns=____, fill_value=0)  # TODO: Fill the blanks to correctly set up the pivot table parameters.\n",
        "\n",
        "# Plotting the data\n",
        "pivot_table.plot(kind='bar', stacked=True)\n",
        "plt.title('Health Outcomes by Rectal Temperature Category')\n",
        "plt.xlabel('Temperature Category')\n",
        "plt.ylabel('Count of Outcomes')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title='Health Outcome')\n",
        "plt.tight_layout()  # Adjust layout to not cut off labels\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SNEr0RjO1k5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Naive Bayes Assumption and Its Impact\n",
        "\n",
        "#### Context:\n",
        "The Naive Bayes classifier, while simple and effective, assumes feature independence which may affect its application in complex datasets like \"Predict Horse Health Outcomes.\"\n",
        "\n",
        "#### Task:\n",
        "Evaluate how the Naive Bayes assumption of feature independence affects its performance on a dataset with interdependent features such as surgery, age, pulse, and mucous membrane status.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Review the Assumption:** Consider the independence assumption of Naive Bayes.\n",
        "2. **Assess Potential Impacts:** Reflect on how this assumption might influence the model's effectiveness, especially where features might be interrelated.\n",
        "3. **Select the Most Accurate Statement:** Choose the statement that best describes Naive Bayes' assumption and its impact on the dataset.\n",
        "\n",
        "#### Statements:\n",
        "1. **Naive Bayes treats all features as equally interdependent.** This is suited for datasets with strong feature correlations, potentially making it less effective for independent feature scenarios.\n",
        "   \n",
        "2. **Naive Bayes assumes independence among features.** This could pose challenges if features like pulse and mucous membrane status, which may be related, are not truly independent.\n",
        "   \n",
        "3. **Naive Bayes expects numerical features to behave like categorical variables.** This might complicate handling datasets with a mix of feature types.\n",
        "   \n",
        "4. **Naive Bayes requires features to be independent and may need preprocessing to handle correlations like those between pulse and mucous membrane status.**\n",
        "\n",
        "#### Question:\n",
        "Considering the assumption and potential impacts of Naive Bayes on the horse health outcomes dataset, which statement number is most accurate?\n",
        "\n",
        "#### Options:\n",
        "A) Statement 1\n",
        "\n",
        "B) Statement 2\n",
        "\n",
        "C) Statement 3\n",
        "\n",
        "D) Statement 4"
      ],
      "metadata": {
        "id": "ge2Ock_O6YkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Analyzing Pain and Euthanasia Decisions\n",
        "\n",
        "#### Context:\n",
        "Understanding the decision-making process in veterinary care, particularly euthanasia, can be crucial in managing horse health outcomes. Analyzing how pain levels influence these decisions is particularly significant.\n",
        "\n",
        "#### Task:\n",
        "Using Bayes' Theorem, calculate the probability that a horse is euthanized given it experiences severe pain and assess what this indicates about the role of pain in euthanasia decisions.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Review Given Probabilities:**\n",
        "   - P(Severe Pain) = 0.20\n",
        "   - P(Euthanized) = 0.25\n",
        "   - P(Severe Pain | Euthanized) = 0.50\n",
        "\n",
        "2. **Calculate the Probability:** Use Bayes' Theorem to find P(Euthanized | Severe Pain).\n",
        "\n",
        "3. **Interpret the Result:** Consider what the calculated probability implies about the impact of severe pain on euthanasia decisions.\n",
        "\n",
        "\n",
        "#### Question:\n",
        "Based on the calculation using Bayes' Theorem, what is the probability that a horse is euthanized given that it is experiencing severe pain? What does this probability suggest about the impact of severe pain on the decision to euthanize?\n",
        "\n",
        "#### Options:\n",
        "A) 0.40\n",
        "\n",
        "B) 0.50\n",
        "\n",
        "C) 0.75\n",
        "\n",
        "D) 0.625\n"
      ],
      "metadata": {
        "id": "A7pJUe_b73uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given probabilities\n",
        "P_SeverePain = 0.20\n",
        "P_Euthanized = 0.25\n",
        "P_SeverePain_Euthanized = 0.50\n",
        "\n",
        "# Bayes' Theorem calculation\n",
        "P_Euthanized_SeverePain = _______  # TODO: Calculate the probability using Bayes' Theorem components given above.\n",
        "print(\"Probability of being euthanized given severe pain: {:.3f}\".format(P_Euthanized_SeverePain))"
      ],
      "metadata": {
        "id": "qapca5kA6GF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Evaluating Feature Significance with Chi-square Test\n",
        "\n",
        "#### Context:\n",
        "In statistical analysis for data, the Chi-square test of independence is essential for determining the association between features. This test helps identify which features are statistically significant in relation to the outcome variable in a dataset.\n",
        "\n",
        "#### Task:\n",
        "Perform a Chi-square test of independence on each Categorical feature against the 'outcome' variable to determine their significance in predicting health outcomes of horses. Given the significance level of 0.05, identify which Categoricalfeature is not significantly associated with the outcome.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Perform Chi-square Tests:** Apply the Chi-square test to each Categorical feature against the 'outcome' variable.\n",
        "2. **Interpret Results:** Evaluate the p-values to determine which Categorical feature(s) fail to show a significant association with the outcome, suggesting they may not be useful for predictive modeling in this context.\n",
        "\n",
        "\n",
        "#### Question:\n",
        "Based on a significance level of 0.05, which Categorical feature was found to be not significantly associated with the 'outcome' variable, therefore failing the Chi-square test?\n",
        "\n",
        "#### Options:\n",
        "A) temp_of_extremities\n",
        "\n",
        "B) peripheral_pulse\n",
        "\n",
        "C) pain\n",
        "\n",
        "D) All columns passed"
      ],
      "metadata": {
        "id": "wiPhYuckKxAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.____ import ______  # TODO: Import the necessary function for performing the Chi-square test.\n",
        "\n",
        "threshold = .05\n",
        "\n",
        "print(f'{\"Column\":<25} | Test result')\n",
        "print('----------------------------------------')\n",
        "\n",
        "for column in cat_cols:\n",
        "    # Create a contingency table\n",
        "    contingency_table = pd.________(df[_____], df[_____])  # TODO: Create a contingency table from two categorical columns.\n",
        "\n",
        "    # Perform the Chi-Square test\n",
        "    chi2, p, dof, expected_freq = ______(contingency_table)  # TODO: Apply the imported function to compute the Chi-square statistic and p-value.\n",
        "\n",
        "    print(f'{column:<25} |   ', '\\033[32mPassed' if p < threshold else '\\033[31mFailed', '\\033[0m')"
      ],
      "metadata": {
        "id": "y9PP-lsPX56r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Data Preprocessing for Modelling\n",
        "\n",
        "#### Context:\n",
        "In preparation for building a Gaussian Naive Bayes model to predict horse health outcomes, the initial step involves preprocessing the dataset. This includes dropping less relevant columns and applying appropriate transformations to the remaining data.\n",
        "\n",
        "#### Task:\n",
        "Prepare the data by dropping specified columns and applying transformations suitable for a Gaussian Naive Bayes model.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Drop Columns:**\n",
        "   - Remove the 'id' columns from the dataset.\n",
        "2. **Apply Transformations:**\n",
        "   - Use `StandardScaler` to normalize numerical data.\n",
        "   - Apply `OrdinalEncoder` to encode categorical data that handles unknown values.\n",
        "3. **Split the Data:**\n",
        "   - Use `train_test_split` with a `random_state=0` to divide the data into training and test sets, targeting 'outcome' as the label. `test_size = 0.2`\n",
        "\n",
        "#### Question:\n",
        "After preprocessing, which categorical feature ends up with the highest number of unique ordinal values?\n",
        "\n",
        "#### Options:\n",
        "A) temp_of_extremities\n",
        "\n",
        "B) pain and mucous_membrane\n",
        "\n",
        "C) cp_data and abdomen\n",
        "\n",
        "D) nasogastric_reflux\n"
      ],
      "metadata": {
        "id": "7Xn4wefJRqYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.______ import StandardScaler, OrdinalEncoder  # TODO: Import necessary preprocessing classes.\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_temp = df.drop(['id'], axis=1)\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numerical_cols = df_temp.select_dtypes(include=[_____, _____]).columns.tolist()  # TODO: Specify data types to select numerical (int and float both) columns.\n",
        "categorical_cols = df_temp.select_dtypes(include=[_____]).columns.tolist()  # TODO: Specify the data type for categorical(object).\n",
        "\n",
        "# Remove the target column from the feature lists\n",
        "categorical_cols.remove('outcome')\n",
        "\n",
        "# Prepare target and features\n",
        "X = df_temp.drop('outcome', axis=1)\n",
        "y = df_temp['outcome']\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = _____(X, y, test_size=0.2, random_state=0)  # TODO: Use the appropriate function to split the data.\n",
        "\n",
        "# Create transformers for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', ______(), numerical_cols),  # TODO: Choose the correct transformer for numerical data.\n",
        "        ('cat', ______(), categorical_cols)  # TODO: Choose the correct transformer for categorical data that handles unknown categories.\n",
        "    ])\n",
        "\n",
        "# Apply preprocessing\n",
        "X_train = preprocessor._____(X_train)  # TODO: Fit and Apply the preprocessor to training data.\n",
        "X_test = preprocessor._____(X_test)  # TODO: Apply the preprocessor to test data.\n",
        "\n",
        "\n",
        "print(f'{\"Column\":<25} | Number of unique values')\n",
        "print('------'*10)\n",
        "for i in categorical_cols:\n",
        "    print(f'{i:<25} |   ', f'\\033[32m{df[i].unique()}' , '\\033[0m')\n"
      ],
      "metadata": {
        "id": "rLICqN4eRrpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Bernoulli Naive Bayes Classifier\n",
        "\n",
        "#### Context:\n",
        "Evaluating the performance of the Bernoulli Naive Bayes classifier not only through traditional classification metrics but also by analyzing the types of errors made can offer deeper insights into the model's behavior. A confusion matrix is particularly useful for this purpose as it visualizes the performance of an algorithm by showing the actual versus predicted classifications.\n",
        "\n",
        "#### Task:\n",
        "Generate a confusion matrix for the Bernoulli Naive Bayes model and analyze which outcome categories are most frequently misclassified.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Prepare and Train the Model:**\n",
        "   - Implement the Bernoulli Naive Bayes classifier and train it on the dataset.\n",
        "   - Predict the outcomes using the test set.\n",
        "\n",
        "2. **Generate a Confusion Matrix:**\n",
        "   - Create a confusion matrix to visualize the model’s predictions against the actual values.\n",
        "\n",
        "3. **Analyze Misclassifications:**\n",
        "   - Examine the confusion matrix to determine which category is most frequently misclassified as \"Euthanized.\"\n",
        "\n",
        "#### Question:\n",
        "According to the confusion matrix generated from the Bernoulli Naive Bayes model, which category is most frequently incorrectly predicted as \"Euthanized\"?\n",
        "\n",
        "#### Options:\n",
        "A) Died\n",
        "\n",
        "B) Lived\n",
        "\n",
        "C) Both 'Died' and 'Lived' are equally misclassified as 'Euthanized'\n",
        "\n",
        "D) Neither 'Died' nor 'Lived' is misclassified as 'Euthanized'"
      ],
      "metadata": {
        "id": "y6xy-nOxecZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.____ import BernoulliNB  # TODO: Correctly import the Bernoulli Naive Bayes classifier.\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Encoding the 'outcome' variable\n",
        "outcome_map = {'died': 0, 'euthanized': 1, 'lived': 2}\n",
        "y_train_encoded = y_train.___(outcome_map)  # TODO: Apply the mapping to the 'y_train' series.\n",
        "y_test_encoded = y_test.____(outcome_map)  # TODO: Apply the mapping to the 'y_test' series.\n",
        "\n",
        "# Train Bernoulli Naive Bayes classifier\n",
        "bernoulli_nb = BernoulliNB()\n",
        "bernoulli_nb.____(_____, y_train_encoded)  # TODO: Fit the Bernoulli Naive Bayes model using training data.\n",
        "y_pred_ber = bernoulli_nb.____(X_test)  # TODO: Predict outcomes using the test set.\n",
        "\n",
        "print(\"Bernoulli Naive Bayes Classification Report:\")\n",
        "print(classification_report(_____, _____, target_names=['Died', 'Euthanized', 'Lived']))  # TODO: Fill in the classification report with the appropriate arguments.\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(_____, _____)  # TODO: Provide the correct variables to generate the confusion matrix.\n",
        "categories = ['Died', 'Euthanized', 'Lived']\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix for Bernoulli Naive Bayes Classifier')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ezpJ4zCMedXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Assesments for Naive Bayes-II\n"
      ],
      "metadata": {
        "id": "mNqoWbeoXCjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Mucous Membrane Condition\n",
        "\n",
        "#### Context:\n",
        "The condition of a horse's mucous membrane can provide valuable insights into its circulatory and respiratory health, which are critical factors in determining the overall health status. Analyzing how different conditions of mucous membranes correlate with health outcomes can help in early diagnosis and management of health issues.\n",
        "\n",
        "#### Task:\n",
        "Investigate the relationship between the condition of the mucous membrane and health outcomes in horses.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Group Data by Mucous Membrane Condition:** Use the dataset to group entries by 'mucous_membrane' and 'outcome' to analyze the distribution of health outcomes within each mucous membrane condition category.\n",
        "2. **Visualize and Analyze Data:** Create a bar chart to visualize the count of each health outcome within the different mucous membrane condition categories. This will help identify any significant correlations between mucous membrane condition and health outcomes.\n",
        "\n",
        "#### Question:\n",
        "After analyzing the bar chart which depicts the count of health outcomes within each mucous membrane condition category, identify the correct statement regarding the correlation between mucous membrane condition and health outcomes.\n",
        "\n",
        "#### Options:\n",
        "A) Horses with normal pink mucous membranes tend to have better health outcomes and higher survival rates.\n",
        "\n",
        "B) A pale or cyanotic mucous membrane is strongly associated with a higher survival rate.\n",
        "\n",
        "C) Horses with bright pink mucous membranes are more likely to be euthanized due to severe health issues.\n",
        "\n",
        "D) The color of the mucous membrane has no correlation with the health outcomes of horses.\n",
        "\n"
      ],
      "metadata": {
        "id": "NOXVtFR252TH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping data by mucous membrane condition and outcome for plotting\n",
        "grouped_data = df.____(_______).______()['id'].reset_index()  # TODO: Choose appropriate columns for grouping and a method to summarize the data.\n",
        "grouped_data.columns = ['Mucous Membrane Condition', 'Outcome', 'Count']\n",
        "\n",
        "# Create a pivot table for visualization\n",
        "pivot_table = pd.pivot_table(grouped_data, values=____, index=______, columns=____, fill_value=0)  # TODO: Determine which values will be summarized and how they should be arranged in the pivot table.\n",
        "\n",
        "# Plotting the data\n",
        "pivot_table.plot(kind='bar', stacked=True)\n",
        "plt.title('Health Outcomes by Mucous Membrane Condition')\n",
        "plt.xlabel('Mucous Membrane Condition')\n",
        "plt.ylabel('Count of Outcomes')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Health Outcome')\n",
        "plt.tight_layout()  # Adjust layout to not cut off labels\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PHf8DxM73XGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### Training and Evaluating the Gaussian Model\n",
        "\n",
        "#### Context:\n",
        "After preprocessing the dataset for the \"Predict Health Outcomes of Horses\" and training a Gaussian Naive Bayes model, the next step is to evaluate the classifier’s performance. This involves encoding the target variable and analyzing the model's ability to predict each category.\n",
        "\n",
        "#### Task:\n",
        "Train and evaluate the Gaussian Naive Bayes classifier using the preprocessed features and encoded target. Use the classification report to identify which category the model has difficulty predicting.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Encode the Target Variable:**\n",
        "   - Map the 'outcome' categories to numeric values using the provided dictionary.\n",
        "2. **Model Training:**\n",
        "   - Train the Gaussian Naive Bayes classifier using the encoded training data.\n",
        "3. **Model Evaluation:**\n",
        "   - Evaluate the classifier's performance using the classification report to identify which category is most problematic.\n",
        "\n",
        "#### Question:\n",
        "Which category does the Gaussian Naive Bayes model struggle with the most, according to the classification report?\n",
        "\n",
        "#### Options:\n",
        "A) Died\n",
        "\n",
        "B) Euthanized\n",
        "\n",
        "C) Lived\n",
        "\n",
        "D) All categories have nearly identical recall"
      ],
      "metadata": {
        "id": "rFZ61ZosR7QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.______ import GaussianNB  # TODO: Import the Gaussian Naive Bayes classifier.\n",
        "from sklearn._____ import classification_report, confusion_matrix  # TODO: Import necessary functions for evaluation.\n",
        "\n",
        "# Encoding the 'outcome' variable\n",
        "outcome_map = {'died': 0, 'euthanized': 1, 'lived': 2}\n",
        "y_train_encoded = y_train.___(outcome_map)  # TODO: Apply the mapping to the 'y_train' series.\n",
        "y_test_encoded = y_test.____(outcome_map)  # TODO: Apply the mapping to the 'y_test' series.\n",
        "\n",
        "# Initialize and train Gaussian Naive Bayes classifier\n",
        "model = GaussianNB()\n",
        "model.____(X_train, _____)  # TODO: Fit the model using the training data.\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.____(X_test)  # TODO: Make predictions using the test data.\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(_____, ____, target_names=['Died', 'Euthanized', 'Lived'])  # TODO: Provide the correct variables to generate the classification report.\n",
        "print(report)"
      ],
      "metadata": {
        "id": "ejLaPK40R3rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Cross-Validation with Gaussian\n",
        "\n",
        "#### Context:\n",
        "Cross-validation is a robust technique for assessing the generalizability of a predictive model beyond the specific data used during training. Using 5-fold cross-validation with the Gaussian Naive Bayes classifier provides insight into the model's performance across different subsets of the dataset.\n",
        "\n",
        "#### Task:\n",
        "Apply 5-fold cross-validation using the Gaussian Naive Bayes Classifier on the \"Predict Health Outcomes of Horses\" dataset, utilizing the 'f1_weighted' scoring metric to evaluate the model's performance.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Set Up Cross-Validation:**\n",
        "   - Use the Gaussian Naive Bayes classifier.\n",
        "   - Implement 5-fold cross-validation.\n",
        "   - Apply the 'f1_weighted' metric to measure model performance across folds.\n",
        "\n",
        "2. **Calculate the Average Score:**\n",
        "   - Determine the average 'f1_weighted' score across all cross-validation folds to assess the overall effectiveness of the model.\n",
        "\n",
        "#### Question:\n",
        "1. What is the average 'f1_weighted' score value using 5-fold cross-validation with the Gaussian Naive Bayes Classifier?\n",
        "2. Is this 'f1_weighted' score higher or lower than the F1 score from the previously generated classification report for the category the model struggled with the most?\n",
        "\n",
        "#### Options:\n",
        "A) Mean F1 score: 0.5, Higher\n",
        "\n",
        "B) Mean F1 score: 0.6, Higher\n",
        "\n",
        "C) Mean F1 score: 0.7, Lower\n",
        "\n",
        "D) Mean F1 score: 0.8, Lower"
      ],
      "metadata": {
        "id": "VpitgxmbDZf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn._____ import cross_val_score  # TODO: Import the function to perform cross-validation.\n",
        "\n",
        "# Perform 5-fold cross-validation using 'f1_weighted' as the scoring metric\n",
        "# Review the scoring parameters available in the documentation to ensure correct usage. https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
        "cv_scores = cross_val_score(_____, X_train, ____, ___=5, scoring='f1_weighted')  # TODO: Initialize the classifier, set the number of folds, and specify the scoring metric.\n",
        "\n",
        "# Calculate the average of the cross-validation scores\n",
        "average_cv_score = cv_scores.____()  # TODO: Determine the appropriate method to calculate the mean of the scores.\n",
        "print(f\"Average F1 weighted score across 5 folds: {average_cv_score:.2f}\")"
      ],
      "metadata": {
        "id": "rqF9aGaSYkKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Multinomial Naive Bayes Evaluation\n",
        "\n",
        "#### Context:\n",
        "After training a Gaussian Naive Bayes model on the \"Predict Health Outcomes of Horses\" dataset and obtaining its performance metrics, the next step involves evaluating a Multinomial Naive Bayes model. This evaluation will use the same dataset, but the continuous features will be scaled to `non-negative` values suitable for Multinomial Naive Bayes.\n",
        "\n",
        "#### Task:\n",
        "Evaluate the performance of the Multinomial Naive Bayes classifier after scaling the features to ensure all are non-negative, as required by the model.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Data Preparation:**\n",
        "   - Scale the continuous features using `MinMaxScaler` to ensure all feature values are non-negative.\n",
        "   \n",
        "2. **Model Training and Evaluation:**\n",
        "   - Train a Multinomial Naive Bayes classifier on the scaled data.\n",
        "   - Generate a classification report to assess the model's performance across different outcome categories.\n",
        "\n",
        "#### Question:\n",
        "Based on the classification report for the Multinomial Naive Bayes model, which category has the highest recall, indicating the best performance in correctly identifying true positive cases?\n",
        "\n",
        "#### Options:\n",
        "A) Died\n",
        "\n",
        "B) Euthanized\n",
        "\n",
        "C) Lived\n",
        "\n",
        "D) All categories have nearly identical recall\n"
      ],
      "metadata": {
        "id": "UvfcsGFobjRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.____ import MultinomialNB  # TODO: Correctly import the Multinomial Naive Bayes classifier.\n",
        "from sklearn.____ import MinMaxScaler  # TODO: Correctly import the MinMaxScaler for data normalization.\n",
        "\n",
        "# Initialize MinMaxScaler\n",
        "scaler = _____()  # TODO: Instantiate the scaler.\n",
        "\n",
        "# Scale X_train and X_test\n",
        "X_train_scaled = scaler._____(X_train)  # TODO: Fir and Apply the scaler to train data.\n",
        "X_test_scaled = scaler._____(X_test)  # TODO: Apply the scaler to test data.\n",
        "\n",
        "# Train Multinomial Naive Bayes classifier\n",
        "multinomial_nb = MultinomialNB()\n",
        "multinomial_nb.____(X_train_scaled, _____)  # TODO: Fit the Multinomial Naive Bayes model to the scaled training data.\n",
        "y_pred_multinomial = multinomial_nb._____(____)  # TODO: Predict the test set results with the trained model.\n",
        "\n",
        "print(\"Multinomial Naive Bayes Classification Report:\")\n",
        "print(classification_report(______, _____, target_names=['Died', 'Euthanized', 'Lived']))  # TODO: Provide the correct arguments to generate the classification report."
      ],
      "metadata": {
        "id": "cdenScZNbkF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Optimizing Laplace Smoothing for Gaussian Naive Bayes Classifier\n",
        "\n",
        "#### Context:\n",
        "Laplace smoothing is a technique used to handle the issue of zero probability in Naive Bayes classifiers. While typically more relevant for discrete data in Multinomial and Bernoulli Naive Bayes, experimenting with this parameter in Gaussian Naive Bayes can provide insights into its effect on model robustness, particularly when handling datasets with sparse features or small sample sizes.\n",
        "\n",
        "#### Task:\n",
        "Determine the best Laplace smoothing parameter (alpha) for a Gaussian Naive Bayes classifier applied to the \"Predict Health Outcomes of Horses\" dataset. This involves tuning the alpha parameter to improve model performance, especially in handling data with potential zero-frequency issues.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Setup Parameter Tuning:**\n",
        "   - Use a range of alpha values to determine which provides the best balance between bias and variance.\n",
        "   - Apply cross-validation to evaluate each model configuration under different alpha settings.\n",
        "\n",
        "2. **Evaluate Model Performance:**\n",
        "   - Measure the performance 'f1_weighted' of the Gaussian Naive Bayes classifier for each alpha setting.\n",
        "   - Identify the alpha value that results in the highest average performance across cross-validation folds.\n",
        "\n",
        "3. **Implement and Discuss Results:**\n",
        "   - Implement the model using the optimal alpha.\n",
        "   - Discuss how Laplace smoothing impacts the performance of a Gaussian Naive Bayes classifier.\n",
        "\n",
        "#### Question:\n",
        "After tuning the Laplace smoothing parameter for the Gaussian Naive Bayes model on the \"Predict Health Outcomes of Horses\" dataset, which alpha value resulted in the highest f1_weighted score?\n",
        "\n",
        "#### Options:\n",
        "A) Alpha = 0.0, Accuracy = 65%\n",
        "\n",
        "B) Alpha = 0.8, Accuracy = 64%\n",
        "\n",
        "C) Alpha = 0.5, Accuracy = 75%\n",
        "\n",
        "D) Alpha = 0.3, Accuracy = 53%\n"
      ],
      "metadata": {
        "id": "oQUs2htdVHFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.____ import GridSearchCV  # TODO: Import the necessary class for optimizing hyperparameters.\n",
        "import numpy as np\n",
        "\n",
        "# Define a range of alpha values to test\n",
        "alpha_range = np.linspace(0.0, 1.0, 11)\n",
        "\n",
        "# Setup GridSearchCV to find the best alpha\n",
        "param_grid = {'_____': alpha_range}  # TODO: Specify the parameter name for Laplace smoothing in GaussianNB.\n",
        "gnb = GaussianNB()\n",
        "clf = GridSearchCV(____, param_grid, ___=5, ____='f1_weighted')  # TODO: Fill in the GridSearchCV constructor with the correct model and parameters.\n",
        "clf.____(X_train, y_train)  # TODO: Fit the GridSearchCV to the training data.\n",
        "\n",
        "# Best alpha and its score\n",
        "best_alpha = clf.best_params_[_____]  # TODO: Extract the best alpha value from clf.\n",
        "best_score = clf.best_score_  # TODO: Extract the best score from clf.\n",
        "print(f\"Best alpha: {best_alpha}\")\n",
        "print(f\"Best scoring f1_weighted: {best_score:.2f}\")"
      ],
      "metadata": {
        "id": "qIn5hPQWeWTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Match the Following attributes\n",
        "\n",
        "#### Context:\n",
        "Understanding how different Naive Bayes classifiers calculate feature importance is crucial for interpreting model outcomes and for feature selection. Each classifier type has specific methods or attributes that help in identifying the contribution of each feature towards making a prediction.\n",
        "\n",
        "#### Task:\n",
        "Match the following Naive Bayes models to the correct method or attribute used for determining feature importance, which provides insights such as $( P(x_i \\mid y) $) or the mean of each feature per class. Use the [scikit-learn documentation](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes) to find the parameters that return these values.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Research**: Look into the scikit-learn documentation for Naive Bayes classifiers to identify the specific attributes associated with each model.\n",
        "2. **Match**: Connect each model listed on the left with its corresponding method or attribute on the right that is used to determine feature importance.\n",
        "\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "<meta charset=\"UTF-8\">\n",
        "<title>Match the Following Characteristics</title>\n",
        "<style>\n",
        "  .matching-table {\n",
        "    width: 100%;\n",
        "    border-collapse: collapse;\n",
        "  }\n",
        "  \n",
        "  .matching-table th,\n",
        "  .matching-table td {\n",
        "    border: 1px solid black;\n",
        "    padding: 5px;\n",
        "    text-align: left;\n",
        "  }\n",
        "  \n",
        "  .matching-table th {\n",
        "    background-color: #f2f2f2;\n",
        "  }\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "\n",
        "<table class=\"matching-table\">\n",
        "  <tr>\n",
        "    <th>Models</th>\n",
        "    <th>Characteristics</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>A. Gaussian Naive Bayes\n",
        "</td>\n",
        "    <td>I. feature_log_prob_</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>B. Multinomial Naive Bayes</td>\n",
        "    <td>II. feature_importances_</td>\n",
        "  </tr>  \n",
        "\t<tr>\n",
        "    <td>C. Bernoulli Naive Bayes</td>\n",
        "    <td>III. epsilon_</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td></td>\n",
        "    <td>IV:  theta_</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "</body>\n",
        "</html>\n",
        "\n",
        "#### Options:\n",
        "- A) A-I, B-III, C-II\n",
        "\n",
        "- B) A-IV, B-I, C-I\n",
        "\n",
        "- C) A-IV, B-I, C-II\n",
        "\n",
        "- D) A-I, B-IV, C-I\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hdq6J8w3lGGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Evaluating Feature Importance in Gaussian Naive Bayes Model\n",
        "\n",
        "#### Context:\n",
        "Understanding which features most influence the predictions of a Gaussian Naive Bayes classifier can enhance interpretability and guide further model refinement. This model evaluates feature importance based on the mean values of features conditioned on each class, reflecting the significance of each feature in class discrimination.\n",
        "\n",
        "#### Task:\n",
        "Analyze feature importance in a Gaussian Naive Bayes model trained on a dataset, using the mean of each feature across classes to determine their impact on the model's predictions.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Model Setup and Training:**\n",
        "   - Initialize the Gaussian Naive Bayes model with `var_smoothing` set to 0.3 to stabilize the calculation by adjusting the variance of each feature.\n",
        "   - Train the model using the training sets `X_train` and `y_train`.\n",
        "\n",
        "2. **Feature Importance Calculation:**\n",
        "   - Extract the mean of each feature for each class from the model. These means are critical as Gaussian Naive Bayes assumes features are normally distributed, and the mean of these distributions plays a significant role in class separation.\n",
        "   - Calculate the `absolute` values of these means to focus on the magnitude of feature values irrespective of their direction.\n",
        "\n",
        "3. **Summarize Feature Importance:**\n",
        "   - Aggregate these mean values across all classes to get a single importance score per feature.\n",
        "   - Rank these features based on their importance scores to identify the most influential features.\n",
        "\n",
        "4. **Visualization:**\n",
        "   - Plot these importance scores using a horizontal bar chart to visually compare the significance of each feature.\n",
        "\n",
        "\n",
        "#### Question:\n",
        "After implementing the above steps, which features were identified as the most important based on the mean absolute values of the class conditional distributions?\n",
        "\n",
        "#### Options:\n",
        "A) rectal_temp, age, peristalsis\n",
        "\n",
        "B) lesion_2, pain, cp_data\n",
        "\n",
        "C) nasogastric_reflux, nasogastric_tube, rectal_exam_feces\n",
        "\n",
        "D) total_protein, lesion_1, temp_category\n"
      ],
      "metadata": {
        "id": "0XdrIektahHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Gaussian Naive Bayes model\n",
        "model = GaussianNB(______ = 0.3) # TODO: Set the smoothing parameter to 0.3.\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# TODO: Use the appropriate numpy function to calculate the absolute values of the means\n",
        "feature_importances = np.___(model._____)   # TODO: Access the attribute that stores the class conditional means.\n",
        "\n",
        "# Create a DataFrame to view these importances\n",
        "importance_df = pd.DataFrame(feature_importances, columns=X.columns)\n",
        "\n",
        "# Summing the importances across classes to get an overall importance score for each feature\n",
        "importance_df.loc['Mean Importance'] = importance_df.____(axis=0)  # TODO: Aggregate the means to get a single importance score per feature\n",
        "sorted_importance = importance_df.loc['Mean Importance']._______(ascending=False)   # TODO: Sort the features based on importance\n",
        "\n",
        "print(\"Feature Importance based on absolute means of class conditional distributions:\")\n",
        "print(sorted_importance)\n",
        "\n",
        "# Optionally, visualize these importances\n",
        "import matplotlib.pyplot as plt\n",
        "sorted_importance.plot(kind='barh')\n",
        "plt.title('Feature Importance in Gaussian Naive Bayes')\n",
        "plt.ylabel('Features')\n",
        "plt.xlabel('Importance (mean absolute values across classes)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C60BjqugCnIX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}